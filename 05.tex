\chapter{}

\begin{theorem}
	Let $R$ be a ring and $n\in\N$. Then $J(M_n(R))=M_n(J(R))$. 
\end{theorem}

\begin{proof}
	We first prove that $J(M_n(R))\subseteq M_n(J(R))$. 
	If $J(R)=R$, the theorem is clear. Let us assume that $J(R)\ne R$ and let  
	$J=J(R)$. 
	If $M$ is a simple $R$-module, then $M^n$ is a simple $M_n(R)$-module with the usual multiplication. 
	Let $x=(x_{ij})\in J(M_n(R))$ and $m_1,\dots,m_n\in M$. Then
	\[
		x\colvec{3}{m_1}{\vdots}{m_n}=0.
	\]
	In particular, $x_{ij}\in\Ann_R(M)$ for all $i,j\in\{1,\dots,n\}$. Hence 
	$x\in M_n(J)$. 

	We now prove that $M_n(J)\subseteq J(M_n(R))$. Let 
	\[
		J_1=\begin{pmatrix}
			J & 0 & \cdots & 0\\
			J & 0 & \cdots & 0\\
			\vdots & \vdots & \ddots & \vdots\\
			J & 0 & \cdots & 0
		\end{pmatrix}
		\quad\text{and}\quad
		x=\begin{pmatrix}
			x_1 & 0 & \cdots & 0\\
			x_2 & 0 & \cdots & 0\\
			\vdots & \vdots & \ddots & \vdots\\
			x_n & 0 & \cdots & 0
		\end{pmatrix}\in J_1.
	\]
	Since $x_1$ es quasi-regular, there exists $y_1\in R$ such that $x_1+y_1+x_1y_1=0$.
	If
	\[
		y=\begin{pmatrix}
			y_1 & 0 & \cdots & 0\\
			0 & 0 & \cdots & 0\\
			\vdots & \vdots & \ddots & \vdots\\
			0 & 0 & \cdots & 0
		\end{pmatrix}, 
	\]
	then $u=x+y+xy$ is lower triangular, as  
	\[
		u=\begin{pmatrix}
			0 & 0 & \cdots & 0\\
			x_2y_1 & 0 & \cdots & 0\\
			x_3y_1 & 0 & \cdots & 0\\
			\vdots & \vdots & \ddots & \vdots\\
			x_ny_1 & 0 & \cdots & 0
		\end{pmatrix}.
	\]
	Since  
	$u^n=0$, the element
	\[
	v=-u+u^2-u^3+\cdots+(-1)^{n-1} u^{n-1}
	\]
	is such that 
	$u+v+uv=0$. Thus $x$ is right quasi-regular, as  
	\begin{align*}
		x+(y+v+yv)+x(y+v+yv)&=0,
	\end{align*}
	and therefore $J_1$ is right quasi-regular. Similarly one proves that 
	each $J_i$ is right quasi-regular and hence $J_i\subseteq J(M_n(R))$ for all 
	$i\in\{1,\dots,n\}$. In conclusion, 
	\[
	J_1+\cdots+J_n\subseteq J(M_n(R))
	\]
	and therefore $M_n(J)\subseteq J(M_n(R))$.
\end{proof}

\begin{exercise}
	Let $R$ be a unitary ring. Then  
	\[
	J(R)=\bigcap\{M:\text{$M$ is a left maximal ideal}\}.
	\]
\end{exercise}

\begin{exercise}
\label{xca:Jcon1}
	Let $R$ be a unitary ring. The
	following statements are equivalent: 
	\begin{enumerate}
		\item $x\in J(R)$.
		\item $xM=0$ for all simple $R$-module $M$.
		\item $x\in P$ for all primitive left ideal $P$.
		\item $1+rx$ is invertible for all $r\in R$.
		\item $1+\sum_{i=1}^n r_ixs_i$ is invertible for all $n\in\N$ and all $r_i,s_i\in R$.
		\item $x$ belongs to every left maximal ideal maximal. 
	\end{enumerate}
\end{exercise}

The following exercise is entirely optional. It somewhat shows a recent application of radical rings
to solutions of the celebrated Yang--Baxter equation. 

\begin{exercise}
A pair $(X,r)$ is a \textbf{solution} to the 
Yang--Baxter equation if $X$ is a set and
$r\colon X\times X\to X\times X$ is a bijective map such that  
\[
	(r\times\id)\circ (\id\times r)\circ (r\times\id)
	=(\id\times r)\circ (r\times\id)\circ (\id\times r)
\]
The solution $(X,r)$ is said to be \textbf{involutive} 
if $r^2=\id$. By convention we write 
\[
	r(x,y)=(\sigma_x(y),\tau_y(x)).
\]
The solution $(X,r)$ is said to be \textbf{non-degenerate}  
$\sigma_x\colon X\to X$ and 
$\tau_x\colon X\to X$ are bijective for all $x\in X$.

\begin{enumerate}
    \item Let $X$ be a set and $\sigma\colon X\to X$ be a bijective map. Prove that  
          the pair $(X,r)$, where 
          $r(x,y)=(\sigma(y),\sigma^{-1}(x))$, is an involutive non-degenerate solution. 
\end{enumerate}
Let $R$ be a radical ring. For $x,y\in R$ let 
\begin{align*}
	&\lambda_x(y)=-x+x\circ y=xy+y,\\
	&\mu_y(x)=\lambda_x(y)'\circ x\circ y=(xy+y)'x+x
\end{align*}
Prove the following statements:
\begin{enumerate}
    \setcounter{enumi}{1}
		\item $\lambda\colon (R,\circ)\to\Aut(R,+)$, $x\mapsto
			\lambda_x$, is a group homomorphism.
		\item $\mu\colon (R,\circ)\to\Aut(R,+)$, $y\mapsto\mu_y$,
    		is a group antihomomorphism.
	    \item The map 
    	\[
	        r\colon R\times R\to R\times R,\quad
	        r(x,y)=(\lambda_x(y),\mu_y(x)),
	    \]
	is an involutive non-degenerate solution. 
\end{enumerate}
\end{exercise}

%\begin{exercise}
%	Sea $A$ un anillo radical. Para $a,b\in A$ se define 
%	\[
%		\mu_b(a)=\lambda_a(b)'\circ a\circ b=(ab+b)'a+a.
%	\]
%	Demuestre que la función $\mu\colon (A,\circ)\to\Aut(A,+)$,
%	$b\mapsto\mu_b$, está bien definida y es un antimorfismo de grupos.
%\end{exercise}

\begin{exercise}
    If $D$ is a division ring and $R=D[X_1,\dots,X_n]$, then
    $J(R)=\{0\}$. 
%     Como las unidades de $R$ son los elementos no nulos de $D$,
% 	$J(R)$ es un ideal de $D$. Como $D$ es simple, $J(R)\in\{0,D\}$. Si
% 	$J(R)=D$, entonces existe  $f\in R$ tal que $-1+f+(-1)f=0$ y luego $-1=0$,
% 	una contradicción. Luego $J(R)=0$.
\end{exercise}


\begin{example}
\index{Ring!local}
    A commutative and unitary ring $R$ is \textbf{local} if it contains
    only one maximal ideal. 
	If $R$ is a local ring and $M$ be its maximal ideal, then $J(R)=M$. Some particular cases: 
	\begin{enumerate}
		\item If $K$ is a field and $R=K\left[ [X] \right]$, then $J(R)=(X)$. 
		\item If $p$ is a prime number and $R=\Z/p^n$, then $J(R)=(p)$. 
	\end{enumerate}
\end{example}

We finish the discussion on the Jacobson radical with 
some results in the case of unitary algebras. 

\begin{theorem}
	Let $A$ be a $K$-algebra and $I$ be a subset of $A$. Then $I$ is 
	a left regular maximal ideal of the algebra $A$ if and only if $I$ is 
	a left regular maximal ideal of the ring $A$.
\end{theorem}

\begin{proof}
	Let $I$ be a left regular maximal ideal of the ring $A$. We claim that
	$\lambda I\subseteq I$ for all $\lambda\in K$. Assume that 
	$\lambda I\not\subseteq I$ for some $\lambda$. Then $I+\lambda I$
	is an ideal of the ring $A$ that contains $I$, as 
	\[
	a(I+\lambda I)=aI+a(\lambda I)\subseteq I+\lambda (aI)\subseteq I+\lambda I.
	\]
	Since $I$ is maximal, it follows that $I+\lambda I=A$. 
	The left regularity of $I$ implies that there exists $e\in R$
	such that 
	$a-ae\in I$ for all $a\in A$. Write $e=x+\lambda y$ for $x,y\in
	I$. Then 
	\[
		e^2=e(x+\lambda y)=ex+e(\lambda y)=ex+(\lambda e)y\in I.
	\]
	Since $e-e^2\in I$ and $e^2\in I$, it follows that $e\in I$. Thus $A=I$, as
	$a-ae\in I$ for all $a\in A$, a contradiction.

	Conversely, if $I$ is a left regular maximal ideal of the algebra $A$, then 
	$I$ is a left regular ideal of the ring $A$. We claim that $I$ is maximal. 
	There exists a left regular maximal ideal $M$ of the ring $A$ that contains $I$. Since 
	$M$ is left regular, it follows that $M$ is a left regular maximal ideal of the ring $A$. Thus 
	$M=I$ because $I$ is maximal. 
% 	ejercicio~\ref{xca:Zorn:regular} sabemos que existe un ideal a izquierda
% 	maximal $L$ del anillo $A$ que contiene a $I$. Como $L$ es regular, la
% 	implicación demostrada nos dice que $L$ es un ideal a izquierda maximal y
% 	regular del anillo $A$. Luego $L=I$ por la maximalidad de $I$.
\end{proof}

\begin{exercise}
    Let $A$ be an algebra. Prove that the Jacobson
    radical of the ring $A$ coincides with the Jacobson radical of the algebra $A$. 
\end{exercise}

% \begin{proof}
% 	Es consecuencia del teorema anterior y de que el radical de Jacobson es la
% 	intersección de los ideales a izquierda maximales y regulares.
% \end{proof}

\begin{lemma}
	\label{lemma:algebraico=nil}
	Let $A$ be an algebra with one and let $x\in J(A)$. 
	Then $x$ is algebraic if and only if $x$ is nil. 
\end{lemma}

\begin{proof}
    Since $x$ is algebraic, there exist $a_0,\dots,a_n\in K$ 
    not all zero such that 
    \[
		a_0+a_1x+\cdots+a_nx^n=0.
	\]
	Let $r$ be the smallest integer such that $a_r\ne 0$. Then 
	\[
		x^r(1+b_1x+\cdots+b_mx^m)=0,
	\]
	for some $b_1,\dots,b_m\in K$. Since $1+b_1x+\cdots+b_mx^m$ is a unit by 
	Exercise~\ref{xca:Jcon1}, it follows that $x^r=0$.
\end{proof}

An application:

\begin{proposition}
	\label{pro:algebraica=>Jnil}
	If $A$ is an algebraic algebra with one, then $J(A)$ is the largest nil ideal of $A$.
\end{proposition}

\begin{proof}
	The previous lemma implies that $J(A)$ is a nil ideal. 
	Proposition~\ref{pro:nilJ} now implies that $J(A)$ is the largest nil ideal of $A$. 
\end{proof}

\begin{theorem}[Amitsur]
	\label{thm:Amitsur}
	Let $A$ be a $K$-algebra with one such that $\dim_KA<|K|$ (as cardinals). Then 
	$J(A)$ is the largest nil ideal of $A$. 
\end{theorem}

\begin{proof}
	If $K$ is finite, then $A$ is a finite-dimensional algebra. In particular, $A$ is algebraic and
	hence $J(A)$ is a nil ideal by Proposition~\ref{pro:algebraica=>Jnil}.

	Assume that $K$ is infinite and let $a\in J(A)$. Exercise~\ref{xca:Jcon1} implies that 
	every element of the form 
	$1-\lambda^{-1}a$, $\lambda\in K\setminus\{0\}$, is invertible. Thus  
	\[
		a-\lambda=-\lambda(1-\lambda^{-1}a)
	\]
	is invertible for all $\lambda\in K\setminus\{0\}$. Let
	$S=\{(a-\lambda)^{-1}:\lambda\in K\setminus\{0\}\}$. Since 
	\[
	(a-\lambda)^{-1}=(a-\mu)^{-1}\Longleftrightarrow\lambda=\mu,
	\]
	it follows that $|S|=|K\setminus\{0\}|=|K|>\dim_KA$. Then $S$ 
	is linearly dependent, so there are $\beta_1,\dots,\beta_n\in K$
	not all zero and distinct elements $\lambda_1,\dots,\lambda_n\in K$ such that 
	\begin{equation}
		\label{eq:Amitsur}
		\sum_{i=1}^n \beta_i(a-\lambda_i)^{-1}=0.
	\end{equation}
	Multiplying~\eqref{eq:Amitsur} by $\prod_{i=1}^n(a-\lambda_i)$ we get 
	\[
		\sum_{i=1}^n\beta_i\prod_{j\ne i}(a-\lambda_j)=0.
	\]
	We claim that $a$ is algebraic over $K$. Indeed,  
	\[
		f(X)=\sum_{i=1}^n\beta_i\prod_{j\ne i}(X-\lambda_j)
	\]
	is non-zero, as, for example, if $\beta_1\ne1$, then  
	$f(\lambda_1)=\beta_1(\lambda_1-\lambda_2)\cdots(\lambda_1-\lambda_n)\ne0$
	and $f(a)=0$. Since $a\in J(A)$ is algebraic, it follows
	$a$ is nil by Lemma~\ref{lemma:algebraico=nil}.
\end{proof}

Amitsur's theorem implies the following result. 

\begin{corollary}
	Sea $K$ un cuerpo no numerable y $A$ una $K$-álgebra con base numerable.
	Entonces $J(A)$ es el mayor ideal nil de $A$.
\end{corollary}

% \begin{proof}
% 	Es consecuencia del teorema de Amitsur pues $\dim_KA<|K|$. 
% \end{proof}

We now finish the lecture with two big open problems. 

\begin{openproblem}[Jacobson--Herstein]
\label{prob:Jacobson}
\index{Jacobson conjecture}
\index{Jacobson--Herstein conjecture}
Let $R$ be a noetherian ring. Is then 
\[
\bigcap_{n\geq1}J(R)^n=\{0\}?
\]
\end{openproblem}

Open problem \ref{prob:Jacobson} was originally formulated by Jacobson in 1956 \cite{MR0222106} 
for one-sided noetherian rings. In 1965 Herstein \cite{MR188253} found a counterexample
in the case of one-sided noetherian rings 
and reformulated the conjecture as it appears here. 

\begin{exercise}[Herstein]
Let $D$ be the ring of rationals with odd denominators. Let
$R=\begin{pmatrix}
    D & \Q\\
    0 & \Q
\end{pmatrix}$. Prove that $R$ is right noetherian and 
$J(R)=\begin{pmatrix}
J(D) & \Q\\
0 & 0
\end{pmatrix}$. Prove that 
$J(R)^n\supseteq\begin{pmatrix}0&\Q\\0&0\end{pmatrix}$ and hence $\bigcap_nJ(R)^n$ is non-zero. 
\end{exercise}

The following problem is maybe the most important open 
problem in non-commutative ring theory. 

\begin{openproblem}[K\"othe]
\label{prob:Koethe}
\index{K\"othe conjecture}
Let $R$ be a ring. Is the sum 
of two arbitrary nil left ideals of $R$ is nil?
\end{openproblem}

Open problem~\ref{prob:Koethe} is the well-known K\"othe's conjecture. 
The conjecture was first formulated in 1930, see \cite{MR1545158}. It is known to be true
in several cases. In full generality, the problem is still open. In~\cite{MR306251} 
Krempa proved that
the following statements are equivalent:
\begin{enumerate}
    \item K\"othe's conjecture is true.  
    \item If $R$ is a nil ring, then $R[X]$ is a radical ring. 
    \item If $R$ is a nil ring, then $M_2(R)$ is a nil ring. 
    \item Let $n\geq2$. If $R$ is a nil ring, then $M_n(R)$ is a nil ring. 
\end{enumerate}

In 1956 Amitsur formulated the following conjecture, see for example
\cite{MR0347873}: If $R$ is a nil ring, then $R[X]$ is a nil ring. In~\cite{MR1793911} 
Smoktunowicz found a counterexample to Amitsur's conjecture. 
This counterexample suggests that K\"othe's conjecture might be false. 
A simplification of Smoktunowicz's example
appears in~\cite{MR3169522}. See \cite{MR1879880,MR2275597} for more
information on K\"othe's conjecture and related topics. 

\section*{\S3. Maschke's theorem}

Let $K$ be a field and $G$ be a group. The \textbf{group algebra} $K[G]$ 
is the vector space (over $K$) with basis $\{g:g\in G\}$ 
and the algebra structure given by the multiplication
\[
	\left(\sum_{g\in G}\lambda_gg\right)\left(\sum_{h\in G}\mu_hh\right)
	=\sum_{g,h\in G}\lambda_g\mu_h(gh).
\]
Note that every element of $K[G]$ is a finite sum of the form $\sum_{g\in G}\lambda_gg$.

\begin{exercise}
\label{xc:K[G]notsimple}
    If $G$ is non-trivial, then $K[G]$ is not simple. 
\end{exercise}

\begin{exercise}
	Let $G=C_n$ be the (multiplicative) cyclic group of order $n$. Prove that 
	$K[G]\simeq K[X]/(X^n-1)$. 
\end{exercise}

\begin{exercise}
	Let $G$ be a finitely-generated torsion-free abelian group. Prove that 
	$K[G]$ is a domain. 
\end{exercise}

\begin{exercise}
	Let $G$ be a group and $H$ be a subgroup of $G$. Let $\alpha\in K[H]$. Prove that 
    $\alpha$ is invertible (resp. left zero divisor) in $K[H]$ if and only if 
	$\alpha$ is invertible (resp. left zero divisor) in
	$K[G]$.
\end{exercise}

\begin{exercise}
	Let $G$ be a group and $\alpha=\sum_{g\in G}\lambda_gg\in K[G]$.  
	The \textbf{support} of $\alpha$ is the set 
	\[
		\supp\alpha=\{g\in G:\lambda_g\ne 0\}.
	\]
	Prove that if $g\in G$, then 
	$\supp(g\alpha)=g(\supp\alpha)$ and $\supp(\alpha g)=(\supp\alpha)g$.
\end{exercise}

% El objetivo de esta sección es calcular el radical de Jacobson del álgebra de
% grupo de un grupo finito. Comenzamos con un ejemplo:

\begin{exercise}
	Let $G=C_2=\langle g\rangle\simeq\Z/2$ the (multiplicative) 
	group with two elements. Note that every element of $K[G]$ is of the form
	$a1+bg$ for some $a,b\in K$. Prove the following statements:
	\begin{enumerate}
	    \item If the characteristic of $K$ is different from two, then 
	    \[
		K[G]\to K\times K,
		\quad
		a1+bg\mapsto (a+b,a-b),
	\]
	is an algebra isomorhism. 
	\item If the characteristic of $K$ is two, then 
	\[
	K[G]\to \begin{pmatrix}
			K & K\\
			0 & K
		\end{pmatrix},
		\quad
		a1+bg\mapsto\begin{pmatrix}
			a+b & b\\
			0 & a+b
		\end{pmatrix},
	\]
	is an algebra isomorphism. 
	\end{enumerate}
\end{exercise}

Veamos otros ejemplo un poco más difíciles. La idea a utilizar es la siguiente:
Si $A$ es una $K$-álgebra y $\rho\colon G\to U(A)$ es un morfismo de grupos,
donde $U(A)$ es el grupo de unidades de $A$, entonces la función $K[G]\to A$,
$\sum_{g\in G}\lambda_gg\mapsto\sum_{g\in G}\lambda_g\rho(g)$, es un morfismo
de álgebras.

\begin{exercise}
	Let $G=C_3$ be the (multiplicative) group of three elements. Prove that
	$\R[G]\simeq\R\times\C$.
% 	Escribamos $G=\langle g:g^3=1\rangle$ y sea 
% 	\[
% 		\varphi\colon\R[G]\to\R\times\C,
% 		\quad
% 		g\mapsto (1,\omega),
% 	\]
% 	donde $\omega$ es una raíz cúbica primitiva de la unidad. Entonces
% 	$\varphi$ es inyectivo pues
% 	$0=\varphi(a1+bg+cg^2)=(a+b+c,a+b\omega+c\omega^2)$ implica que $a=b=c=0$.
% 	Luego $\varphi$ es un isomorfismo pues
% 	$\dim_\R\R[G]=\dim_\R(\R\times\C)=3$. 
\end{exercise}

\begin{exercise}
	Let $G=\langle r,s:r^3=s^2=1,\,srs=r^{-1}\rangle$ be the dihedral group of six elements. 
	Prove the following statements:
	\begin{enumerate}
	    \item $\C[G]\simeq\C\times\C\times M_2(\C)$.
	    \item $\Q[G]\simeq\Q\times\Q\times M_2(\Q)$.
	\end{enumerate}  
% 	Sea $\omega$ una raíz cúbica de la unidad y sean  
% 	\[
% 		R=\begin{pmatrix}
% 			\omega & 0\\
% 			0 & \omega^2
% 		\end{pmatrix},
% 		\quad
% 		S=\begin{pmatrix}
% 			0 & 1\\
% 			1 & 0
% 		\end{pmatrix}.
% 	\]
% 	Un cálculo sencillo muestra que $R^2=S^2=I$ y que $SRS=R^{-1}$. Sea
% 	\[
% 		\varphi\colon\C[G]\to\C\times\C\times M_2(\C),\quad
% 		r\mapsto (1,1,R),\quad
% 		s\mapsto (1,-1,S).
% 	\]
% 	Es fácil ver que $\varphi$ es un morfismo de álgebras. Veamos que es
% 	biyectivo. Como $\dim_{\C}\C[G]=\dim_{\C}(\C\times\C\times M_2(\C))=6$,
% 	basta ver que $\varphi$ es inyectivo. Si 
% 	\[
% 		\alpha=a_0+a_1r+a_2r^2+(b_0+b_1r+b_2r^2)s\in\ker\varphi,
% 	\]
% 	entonces 
% 	\[
% 		0=\varphi(\alpha)=\left(u,v,\begin{pmatrix} \alpha_{11} & \alpha_{12}\\\alpha_{21}&\alpha_{22}\end{pmatrix}\right), 
% 	\]
% 	donde
% 	\begin{align*}
% 		&u = a_0+a_1+a_2+b_0+b_1+b_2, && v = a_0+a_1+a_2-b_0-b_1-b_2,\\
% 		&\alpha_{11}=a_0+a_1\omega+a_2\omega^2, && \alpha_{12}=b_0+b_1\omega+b_2\omega^2,\\
% 		&\alpha_{21}=b_0+b_2\omega+b_1\omega^2, && \alpha_{22}=a_0+a_2\omega+a_1\omega^2.
% 	\end{align*}
% 	Un cálculo sencillo muestra que estas ecuaciones implican que
% 	$\alpha=0$ y luego $\varphi$ es inyectiva.  
\end{exercise}

\begin{theorem}[Maschke]
	Let $G$ be a finite group. Then $J(K[G])=0$ if and only if the characteristic of $K$ is zero 
	or does not divide the order of $G$. 
\end{theorem}

\begin{proof}
	Supongamos que $G=\{g_1,\dots,g_n\}$ con $g_1=1$. Sea $\rho\colon K[G]\to
	K$ dada por $\alpha\mapsto\trace(L_{\alpha})$, donde
	$L_{\alpha}(\beta)=\alpha\beta$. Tenemos $\rho(g_1)=n$ y $\rho(g_i)=0$ para
	todo $i\in\{2,\dots,n\}$ pues,  como $L_{g_i}(g_j)=g_{i}g_j\ne g_j$, la
	matriz de $L_{g_i}$ en la base $\{g_1,\dots,g_n\}$ tiene ceros en la
	diagonal.

	Supongamos que $J=J(K[G])$ es no nulo y sea
	$\alpha=\sum_{i=1}^n\lambda_ig_i\in J\setminus\{0\}$. Sin pérdida de
	generalidad podemos suponer que $\lambda_1\ne 0$ (pues si $\lambda_1=0$ hay
	algún $\lambda_i\ne 0$ y alcanza con tomar $g_i^{-1}\alpha\in J$). Entonces
	\[
		\rho(\alpha)=\sum_{i=1}^n \lambda_i\rho(g_i)=n\lambda_1.
	\]
	Como $G$ es un grupo finito, $K[G]$ es un álgebra de dimensión finita y
	luego $K[G]$ es artiniana a izquierda. Como el radical de Jacobson $J$ es
	un ideal nilpotente, en particular $\alpha$ es un elemento nil. Luego
	$L_{\alpha}$ es nilpotente y entonces $0=\rho(\alpha)=n\lambda_1$. Esto
	implica que la característica del cuerpo $K$ divide a $n$. 

	Recíprocamente, supongamos que la característica de $K$ es un número primo
	que divide a $n$ y sea $\alpha=\sum_{i=1}^ng_i$. Como $\alpha
	g_j=g_j\alpha=\alpha$ para todo $j\in\{1,\dots,n\}$, el conjunto
	$I=K[G]\alpha$ es un ideal de $K[G]$. Como además 
	\[
		\alpha^2=\sum_{i=1}^n g_i\alpha=n\alpha=0,
	\]
	se concluye que $I$ es un ideal no nulo y nilpotente. Luego $J(K[G])\ne 0$
	pues por la proposición~\ref{pro:nilJ} sabemos que $I\subseteq J(K[G])$.
\end{proof}

\begin{corollary}
	\label{cor:GfinitoNOnil}
	Sea $G$ un grupo finito. Entonces $K[G]$ no contiene ideales a izquierda
	nil no nulos.
\end{corollary}

\begin{proof}
	Es consecuencia inmediata del teorema de Maschke ya que $J(K[G])$ contiene a
	todo ideal a izquierda nil.	
\end{proof}

%\index{Anillo!semisimple}
%Recordemos que un anillo unitario $R$ se dice \textbf{semisimple} si para cada
%ideal $I$ de $R$ existe un ideal $J$ de $R$ tal que $R=I\oplus J$.
%
%%\begin{corollary}
%%	Sea $G$ un grupo finito y $K$ un cuerpo de característica coprima con el
%%	orden de $G$. Entonces $K[G]$ es semisimple.
%%\end{corollary}
%%
%%\begin{proof}
%%	
%%\end{proof}
%
%\begin{theorem}
%	Si $G$ es un grupo infinito, entonces $K[G]$ nunca es semisimple.
%\end{theorem}
%
%\begin{proof}
%	Sea $R=K[G]$ y supongamos que $R$ es semisimple.  Si $I$ es el ideal de
%	aumentación de $R$, existe un ideal no nulo $J$ de $R$ tal que $R=I\oplus
%	J$. Como $R$ es unitario, existen $e\in I$, $f\in J$ tales que $1=e+f$. Si
%	$x\in I$, entonces $x=xe+xf$ y luego $xf=x-xe\in I\cap J=\{0\}$. Como
%	entonces $x=xe$ para todo $x\in I$, en particular $e_1=e_1^2$. Análogamente
%	vemos que $e_2^2=e_2$. Además $ef=0$ pues $ef\in I\cap J=\{0\}$.  Como $I$
%	es el ideal de aumentación y $If=(Re)f=R(ef)=0$, se concluye que $(g-1)f=0$
%	para todo $g\in G$ pues $g-1\in I$. Si suponemos que $f=\sum_{h\in
%	G}\lambda_hh$, entonces 
%	\[
%	f=gf=\sum_{h\in G}\lambda_h(gh)=\sum_{h\in
%	G}\lambda_{g^{-1}h}h.
%	\]
%	Luego $\lambda_h=\lambda_{g^{-1}h}$ para todo $g,h\in G$, una contradicción
%	pues como $f\ne 0$ la suma que define a $f$ es infinita. 
%\end{proof}


\section*{Herstein's theorem}

El objetivo de esta sección responderemos la siguiente pregunta: ¿Cuándo un
álgebra de grupo es un álgebra algebraica? Una respuesta parcial está dada por
el teorema de Herstein. 

\begin{definition}
	\index{Grupo!localmente finito}
	Un grupo $G$ se dice \textbf{localmente finito} si todo subgrupo de $G$
	finitamente generado es finito.
\end{definition}

Si $G$ es un grupo localmente finito, entonces todo $g\in G$ tiene orden finito
(pues el subgrupo $\langle g\rangle$ es finito por ser finitamente generado). 

\begin{example}
	Todo grupo finito es obviamente localmente finito.
\end{example}

\begin{example}
	El grupo $\Z$ no es localmente finito pues es libre de torsión.
\end{example}

\begin{example}
	\index{Grupo!de Pr\"ufer}
	Sea $p$ un primo.  El \textbf{grupo de Pr\"ufer}
	\[
		\Z(p^{\infty})=\{z\in\Z:z^{p^n}=1\text{ para algún $n\in\N$}\}
	\]
	de todas
	las raíces $p$-ésimas de uno es localmente finito.
\end{example}

\begin{example}
	Sean $X$ un conjunto infinito y $\Sym_X$ el conjunto de biyecciones $X\to
	X$ que mueven únicamente una cantidad finita de elementos de $X$. Entonces
	$\Sym_X$ es localmente finito.
\end{example}

Antes de demostrar el teorema de Herstein vamos a dar una familia de ejemplos
de grupos localmente finitos. Para eso necesitamos un lema:

\begin{lemma}
	\label{lem:solvable_torsion=>lf}
	Sea $G$ un grupo y sea $N$ un subgrupo normal de $G$. Si $N$ y $G/N$ son
	localmente finitos, entonces $G$ es localmente finito.
\end{lemma}

\begin{proof}
	Sea $\pi\colon G\to G/N$ el morfismo canónico. Sea $\{g_1,\dots,g_n\}$ un
	subconjunto finito de $G$. Como $G/N$ es localmente finito, el subgrupo $Q$
	de $G/N$ generado por $\pi(g_1),\dots,\pi(g_n)$ es finito, digamos
	\[
		Q=\{\pi(g_1),\dots,\pi(g_n),\pi(g_{n+1}),\dots,\pi(g_m)\}.
	\]
	Para cada $i,j\in\{1,\dots,n\}$ sabemos que existen $u_{ij}\in N$ y
	$k\in\{1,\dots,m\}$ tales que $g_ig_j=u_{ij}g_k$. Sea $U$ el subgrupo de
	$G$ generado por los $u_{ij}$. Como $N$ es localmente finito, $U$ es un
	subgrupo finito. Como además cada elemento $g_ig_jg_l$ puede escribirse como
	\[
		g_ig_jg_l=u_{ij}g_kg_l=u_{ij}u_{kl}g_t=ug_t
	\]
	para algún $u\in U$ y algún $t\in\{1,\dots,m\}$, se concluye que el
	subgrupo $H$ de $G$ generado por $\{g_1,\dots,g_n\}$ es finito pues
	$|H|\leq m|U|$. 
\end{proof}

\index{Grupo!resoluble}
Veamos una aplicación a los grupos resolubles. Recordemos que un grupo $G$ se
dice \textbf{resoluble} si existe una sucesión de subgrupos 
\begin{equation}
	\label{eq:resoluble}
	1=G_0\subsetneq G_1\subsetneq \cdots\subsetneq G_n=G
\end{equation}
donde cada $G_i$ es normal en $G_{i+1}$ y cada cociente $G_i/G_{i-1}$ es
abeliano.

\begin{proposition}
	Si $G$ es un grupo resoluble y de torsión, entonces $G$ es localmente
	finito.
\end{proposition}

\begin{proof}
	Procederemos por inducción en la longitud $n$ de la sucesión de
	resolubilidad~\eqref{eq:resoluble}. Si $n=1$ entonces $G$ es finito por ser
	abeliano y de torsión. Supongamos que el resultado vale para grupos
	resolubles de longitud $n-1$ y sea $G$ un grupo resoluble tal
	que~\eqref{eq:resoluble}. Por hipótesis inductiva, el subgrupo normal
	$G_{n-1}$ de $G$ es localmente finito. Entonces, como $G/G_{n-1}$ es
	localmente finito por ser abeliano y de torsión, el resultado se obtiene
	del lema~\ref{lem:solvable_torsion=>lf}.
\end{proof}

\begin{theorem}[Herstein]
	\index{Teorema!de Herstein}
	Si $G$ es un grupo localmente finito, entonces $K[G]$ es algebraica.
	Recíprocamente, si $K[G]$ es algebraica y $K$ es de característica cero,
	entonces $G$ es localmente finito.
\end{theorem}

\begin{proof}
	Supongamos que $G$ es localmente finito y sea $\alpha\in K[G]$. El subgrupo
	$H=\langle\supp\alpha\rangle$ es finitamente generado y luego finito. Como
	$\alpha\in K[H]$ y $\dim_KK[H]<\infty$, el conjunto
	$\{1,\alpha,\alpha^2,\dots\}$ es linealmente dependiente. Luego $\alpha$ es
	algebraico sobre $K$.

	Sea $\{x_1,\dots,x_m\}$ un subconjunto finito de $G$. Si agregamos los
	inversos, podemos suponer que $\{x_1,\dots,x_m\}$ genera al subgrupo
	$H=\langle x_1,\dots,x_m\rangle$ como semigrupo. Si
	$\alpha=x_1+\dots+x_m\in K[G]$, entonces, como $\alpha$ es algebraico sobre
	$K$, 
	\[
		\alpha^{n+1}=a_0+a_1\alpha+\cdots+a_n\alpha^n
	\]
	para algún $n\geq0$ y escalares $a_0,\dots,x_n\in K$. Sea $w=x_{i_1}\cdots
	x_{i_{n+1}}\in H$ una palabra de longitud $n+1$. Observemos que existen enteros
	positivos $c_{i_1\cdots i_m}$ tales que 
	\[
		\alpha^{n+1}=(x_1+\cdots+x_m)^{n+1}
		=\sum_{\substack{{i_1+\cdots+i_m=n+1}\\{\text{$i_j$ enteros positivos}}}} c_{i_1\cdots i_m}x_1^{i_1}\cdots x_{m}^{i_m}.
	\]
	Como $K$
	es de característica cero, se concluye que $w\in\supp(\alpha^{n+1})$.  Pero
	como además $\alpha^{n+1}=\sum_{j=0}^na_j\alpha^j$, entonces
	$w\in\supp(\alpha^j)$ para algún $j\in\{0,\dots,n\}$. Demostramos entonces
	que toda palabra en las $x_j$ de longitud $n+1$ puede escribirse como una
	palabra en las $x_j$ de longitud a lo sumo $n$.  Luego $H$ es finito y
	entonces $G$ es localmente finito.
\end{proof}

\section*{Formanek's theorem}

Veremos un resultado de Formanek que puede entenderse como una generalización
del teorema de Herstein. 

\begin{exercise}
	Sea $A$ un álgebra algebraica y sea $a\in A$. Demuestre las siguientes
	afirmaciones:
	\begin{enumerate}
		\item $a$ es un divisor de cero a izquierda si y sólo si $a$ es un
			divisor de cero a derecha.
		\item $a$ es inversible a izquierda si y sólo si $a$ es inversible a
			derecha.
		\item $a$ es inversible si y sólo si $a$ no es un divisor de cero.
	\end{enumerate}
\end{exercise}

%\begin{proof}
%	Como $a$ es algebraico, podemos escribir 
%	\[
%		a^n(1+\lambda_1a+\cdots+\lambda_ma^m)=0
%	\]
%	para algún $n\geq0$ minimal y escalares $\lambda_1,\dots,\lambda_m$. Si 
%	$n>0$, entonces 
%	\[
%	b=(1+\lambda_1a+\cdots+\lambda_ma^m)a^{n-1}\ne 0
%	\]
%	cumple que $ab=ba=0$. Si $n=0$, entonces 
%	\[
%		c=-\lambda_1-\lambda_2a-\cdots-\lambda_ma^{m-1}\ne 0
%	\]
%	cumple que $ac=ca=1$. 
%\end{proof}

\begin{exercise}
	\label{exa:norma}
	Si $\alpha=\sum_{g\in G}\alpha_gg\in\C[G]$ se define $|\alpha|=\sum_{g\in
	G}|\alpha_g|\in\R$. Demuestre que valen las siguientes propiedades:
	\begin{enumerate}
		%\item $|\trace(\alpha)|\leq |\alpha|$, 
		\item $|\alpha+\beta|\leq|\alpha|+|\beta|$, y 
		\item $|\alpha\beta|\leq|\alpha||\beta|$ 
	\end{enumerate}
	para todo $\alpha,\beta\in\C[G]$.
\end{exercise}

\begin{theorem}[Formanek, primera versión]
	\label{thm:FormanekQ}
	\index{Teorema!de Formanek}
	Sea $G$ un grupo y supongamos que todo elemento de $\Q[G]$ es inversible o
	un divisor de cero. Entonces $G$ es localmente finito.
\end{theorem}

\begin{proof}
	Sea $\{x_1,\dots,x_n\}$ un subconjunto finito de $G$. Si agregamos los
	inversos, podemos suponer que $\{x_1,\dots,x_n\}$ genera al subgrupo
	$H=\langle x_1,\dots,x_n\rangle$ como semigrupo. Sea
	\[
		\alpha=\frac{1}{2n}(x_1+\cdots+x_n)\in\Q[G]
	\]

	Veamos que $1-\alpha\in\Q[G]$ es inversible. Si no, entonces es un divisor de cero. 
	Si existe $\delta\in\Q[G]$ tal que $\delta(1-\alpha)=0$, entonces
	$\delta=\delta\alpha$ y luego, como 
	\[
		|\delta|=|\delta\alpha|\leq|\delta||\alpha|=|\delta|/2,
	\]
	se concluye que $\delta=0$. Similarmente se demuestra que $(1-\alpha)\delta=0$ implica que
	$\delta=0$. 
	
	Sea $\beta=(1-\alpha)^{-1}\in\Q[G]$.  Para cada $k$ definimos 
	\[
		\gamma_k=(1+\alpha+\cdots+\alpha^k)-\beta.
	\]
	Entonces 
	\begin{align*}
		\gamma_k(1-\alpha)&=(1+\alpha+\cdots+\alpha^k-\beta)(1-\alpha)\\
		&=(1+\alpha+\cdots+\alpha^k)(1-\alpha)-\beta(1-\alpha)=-\alpha^{k+1}
	\end{align*}
	y luego 
	$\gamma_k=-\alpha^{k+1}\beta$. Como 
	\[
		|\gamma_k|=|-\alpha^{k+1}\beta|\leq|\beta||\alpha^{k+1}|=\frac{|\beta|}{2^{k+1}},
	\]
	se concluye que $\lim_{k\to\infty}|\gamma_k|=0$. 

	Para terminar veamos que $H\subseteq\supp\beta$. Si
	$H\not\subseteq\supp\beta$, sea $h\in H\setminus\supp\beta$.  Supongamos
	que $h=x_{i_1}\cdots x_{i_m}$ es una palabra de longitud $m$ en los $x_j$.
	Sea $c_j$ el coeficiente de $h$ en $\alpha^j$. Entonces $c_0+\cdots+c_k$ es
	el coeficiente de $h$ en $\gamma_k$, pero
	\[
		|\gamma_k|\geq c_0+c_1+\cdots+c_k\geq c_m>0
	\]
	para todo $k\geq m$ pues cada $c_j$ es no negativo, una contradicción pues
	demostramos que $|\gamma_k|\to 0$ si $k\to\infty$.
\end{proof}

A continuación explicaremos por qué el teorema de Formanek se considera una
generalización del teorema de Herstein. En el teorema~\ref{thm:FormanekQ} nos
concentramos en álgebras de grupo sobre los números racionales. ¿Cómo podemos
extender este resultado a álgebras de grupo sobre cuerpos de característica
cero? Para extender el cuerpo de base sobre el que se trabaja necesitamos
definir el producto tensorial de espacios vectoriales y el producto tensorial
de álgebras.

\begin{definition}
	\index{Producto tensorial!de espacios vectoriales}
	El \textbf{producto tensorial} de los $K$-espacios vectoriales $U$ y $V$ es
	el espacio vectorial cociente $K[U\times V]/T$, donde $K[U\times V]$ es el
	espacio vectorial con base $\{(u,v):u\in U,v\in V\}$ y $T$ es el subespacio
	generado por los elementos de la forma
	\[
		(\lambda u+\mu u',v)-\lambda(u,v)-\mu(u',v),\quad
		(u,\lambda v+\mu v')-\lambda(u,v)-\mu(u,v')
	\]
	para $\lambda,\mu\in K$, $u,u'\in U$ y $v,v'\in V$.
\end{definition}

El producto tensorial de $U$ y $V$ será denotado por $U\otimes_KV$ o por
$U\otimes V$ si la referencia al cuerpo $K$ puede omitirse. Dados $u\in U$
y $v\in V$ escribiremos $u\otimes v$ para denotar a la coclase $(u,v)+T$.

\begin{theorem}
\index{Producto tensorial!propiedad universal}
	Sean $U$ y $V$ espacios vectoriales.  Existe entonces una función bilineal
	$U\times V\to U\otimes V$, $(u,v)\mapsto u\otimes v$, tal que todo
	elemento de $U\otimes V$ es una suma finita de la forma
	\[
		\sum_{i=1}^N u_i\otimes v_i
	\]
	para $u_1,\dots,u_N\in U$ y $v_1,\dots,v_N\in V$. 
	Más aún, dado un espacio vectorial $W$ y una función
	bilineal $\beta\colon U\times V\to W$, existe una función lineal
	$\overline{\beta}\colon U\otimes V\to W$ tal que $\overline{\beta}(u\otimes
	v)=\beta(u,v)$ para todo $u\in U$ y $v\in V$.
\end{theorem}

\begin{proof}
	Por la definición del producto tensorial, la función 
	\[
	U\times V\to U\otimes V,\quad
	(u,v)\mapsto u\otimes v,
	\]
	es bilineal. También de la definición se deduce inmediatamente que todo
	elemento de $U\otimes V$ es una combinación lineal finita de elementos de
	la forma $u\otimes v$, donde $u\in U$ y $v\in V$. Como $\lambda(u\otimes
	v)=(\lambda u)\otimes v$ para todo $\lambda\in K$, la primera afirmación
	queda demostrada.

	Como $U\times V$ es base de $K[U\times V]$, existe una transformación lineal 
	\[
		\gamma\colon K[U\times V]\to W,\quad
	\gamma(u,v)=\beta(u,v). 
	\]
	Como $\beta$ es bilineal por hipótesis, $T\subseteq\ker\gamma$. Existe
	entonces una transformación lineal $\overline{\beta}\colon U\otimes V\to
	W$ tal que 
	\[
	\begin{tikzcd}
		K[U\times V] \arrow[r]\arrow[d] & W \\
		U\otimes V\arrow[ur, dashrightarrow]
	\end{tikzcd}
	\]
	conmuta. En particular, $\overline{\beta}(u\otimes v)=\beta(u,v)$. 
\end{proof}

\begin{exercise}
	\label{xca:tensorial_unicidad}
	Demuestre que las propiedades mencionadas en el teorema anterior
	caracterizan el producto tensorial salvo isomorfismo.
\end{exercise}

Veamos algunas propiedades del producto tensorial de espacios vectoriales. 
%Observemos
%que todo elemento de $U\otimes V$ es una suma finita
%de la forma 
%\[
%	\sum_{i=1}^N u_i\otimes v_i
%\]
%para $N\in\N$, $u_i\in U$ y $v_i\in V$. Esta expresión no es única. Vale además
%que $u\otimes 0=0=0\otimes v$ para todo $u\in U$ y $v\in V$.

\begin{lemma}
	\index{Producto tensorial!de transformaciones lineales}
	Sean $\varphi\colon U\to U'$ y $\psi\colon V\to V'$ transformaciones
	lineales. Existe entonces una única transformación lineal
	$\varphi\otimes\psi\colon U\otimes V\to U'\otimes V'$ tal que
	\[
		(\varphi\otimes\psi)(u\otimes v)=\varphi(u)\otimes\psi(v)
	\]
	para todo $u\in U$ y $v\in V$.
\end{lemma}

\begin{proof}
	Como la función $U\times V\to U\otimes V$,
	$(u,v)\mapsto\varphi(u)\otimes\psi(v)$, es bilineal, existe una
	transformación lineal $U\otimes V\to U\otimes V$, $u\otimes
	v\to\varphi(u)\otimes\psi(v)$. Luego la función
	\[
		\sum u_i\otimes v_i\mapsto\sum\varphi(u_i)\otimes\psi(v_i)
	\]
	está bien definida. 
\end{proof}

\begin{exercise}
	Demuestre las siguientes afirmaciones:
	\begin{enumerate}
		\item $(\varphi\otimes\psi)(\varphi'\otimes\psi')=(\varphi\varphi')\otimes(\psi\psi')$.
		\item Si $\varphi$ y $\psi$ son isomorfismos, entonces
			$\varphi\otimes\psi$ es un isomorfismo. 
		\item $(\lambda\varphi+\lambda'\varphi')\otimes\psi=\lambda\varphi\otimes\psi+\lambda'\varphi'\otimes\psi$.
		\item $\varphi\otimes(\lambda\psi+\lambda'\psi')=\lambda\varphi\otimes\psi+\lambda'\varphi\otimes\psi'$.
		\item Si $U\simeq U'$ y $V\simeq V'$, entonces $U\otimes V\simeq U'\otimes V'$.
	\end{enumerate}
\end{exercise}

\begin{lemma}
	Si $U$ y $V$ son espacios vectoriales, entonces 
	$U\otimes V\simeq V\otimes U$.
\end{lemma}

\begin{proof}
	Como la función $U\times V\to V\otimes U$, $(u,v)\mapsto v\otimes u$,
	existe una transformación lineal $U\otimes V\to V\otimes U$, $u\otimes
	v\mapsto v\otimes u$. Similarmente se demuestra que existe una
	transformación lineal $V\otimes U\to U\otimes V$, $v\otimes u\mapsto
	u\otimes v$. Luego $U\otimes V\simeq V\otimes U$.
\end{proof}

\begin{exercise}
	\label{xca:UxVxW}
	Demuestre que $(U\otimes V)\otimes W\simeq U\otimes(V\otimes W)$.
\end{exercise}

\begin{exercise}
	\label{xca:UxK}
	Demuestre que $U\otimes K\simeq K\simeq K\otimes U$.
\end{exercise}

\begin{lemma}
	\label{lem:U_LI}
	Sea $\{u_1,\dots,u_n\}\subseteq U$ un conjunto linealmente independiente y
	sean $v_1,\dots,v_n\in V$ tales que $\sum_{i=1}^n u_i\otimes v_i=0$.
	Entonces $v_i=0$ para todo $i\in\{1,\dots,n\}$.
\end{lemma}

\begin{proof}
	Sea $i\in\{1,\dots,n\}$ y sea $f_i\colon U\to K$, $f_i(u_j)=\delta_{ij}$.
	Como la función $U\times V\to V$, $(u,v)\mapsto f_i(u)v$, es bilineal, existe una función
	$\alpha_i\colon U\otimes V\to V$ lineal tal que $\alpha_i(u\otimes
	v)=f_i(u)v$. Luego
	\[
		v_i=\sum_{j=1}^n\alpha_i(u_j\otimes v_j)=\alpha_i\left(\sum_{j=1}^nu_j\otimes v_j\right)=0.
	\]
\end{proof}

\begin{exercise}
	\label{xca:uxv=0}
	Demuestre que si $u\otimes v=0$ y $v\ne 0$, entonces $u=0$.
\end{exercise}

\begin{theorem}
	Si $\{u_i:i\in I\}$ es una base de $U$ y $\{v_j:j\in J\}$ es una base de
	$V$, entonces $\{u_i\otimes v_j:i\in I,j\in J\}$ es una base de $U\otimes
	V$.
\end{theorem}

\begin{proof}
	Los $u_i\otimes v_j$ forman un conjunto de generadores pues  
	si $u=\sum_i\lambda_iu_i$ y $v=\sum_j\mu_jv_j$, entonces
	$u\otimes v=\sum_{i,j}\lambda_i\mu_ju_i\otimes v_j$. 
	Veamos ahora que los $u_i\otimes v_j$ son linealmente independientes. Para
	eso, queremos ver que cualquier subconjunto finito de los $u_i\otimes v_j$
	es linealmente independiente. Si $\sum_k\sum_l\lambda_{kl}u_{i_k}\otimes
	v_{j_l}=0$, entonces
	$0=\sum_{k}u_{i_k}\otimes\left(\sum_{l}\lambda_{kl}v_{j_l}\right)$ y luego,
	como los $u_{i_k}$ son linealmente indepentientes, el lema~\ref{lem:U_LI}
	implica que $\sum_{l}\lambda_{kl}v_{j_l}=0$. Luego $\lambda_{kl}=0$ para
	todo $k,l$ pues los $v_{j_l}$ son linealmente independientes.
\end{proof}

El teorema anterior implica inmediatamente que si $U$ y $V$ son espacios
vectoriales de dimensión finita entonces
\[
	\dim(U\otimes V)=(\dim U)(\dim V).
\]

\begin{corollary}
	Si $\{u_i:i\in I\}$ es base de $U$, entonces todo elemento de $U\otimes V$
	se escribe unívocamente como una suma finita $\sum_{i}u_i\otimes v_i$.
\end{corollary}

\begin{proof}
	Sabemos que todo elemento de $U\otimes V$ es una suma finita
	$\sum_i x_i\otimes y_i$, donde $x_i\in U$ y $y_i\in V$. Si escribimos 
	$x_i=\sum_j\lambda_{ij}u_j$, entonces
	\[
		\sum_i x_i\otimes y_i=\sum_i\left(\sum_j\lambda_{ij}u_j\right)\otimes y_i		
		=\sum_j u_j\otimes\left(\sum_i\lambda_{ij}y_i\right).
	\]
\end{proof}

%\begin{corollary}
%	Todo elemento no nulo de $U\otimes V$ puede escribirse como una suma finita
%	$\sum_{i=1}^N u_i\otimes v_i$ para un conjuntos $\{u_i:1\leq i\leq
%	N\}\subseteq U$ y $\{v_i:1\leq i\leq N\}\subseteq V$ linealmente
%	independientes.
%\end{corollary}
%
%\begin{proof}
%	tomar $N$ minimal	
%\end{proof}

\index{Producto tensorial!de álgebras}
El siguiente lema nos permite definir el \textbf{producto tensorial de
álgebras}.

\begin{lemma}
	Si $A$ y $B$ son álgebras, entonces $A\otimes B$ es un álgebra con el
	producto
	\[
		(a\otimes b)(x\otimes y)=ax\otimes by.
	\]
\end{lemma}

\begin{proof}
	Para $x\in A$, $y\in B$ consideramos $R_x\otimes R_y\in\End_K(A\otimes B)$.
	Como la función $A\times B\to\End_K(A\otimes B)$, $(x,y)\mapsto R_x\otimes
	R_y$, es bilineal, existe una función lineal $\varphi\colon A\otimes
	B\to\End_K(A\otimes B)$, $\varphi(x\otimes y)=R_x\otimes R_y$. Para $u,v\in A\otimes B$ definimos
	\[
		uv=\varphi(v)(u).
	\]
	Esta operación es bilineal pues por ejemplo
	\[
		u(v+w)=\varphi(v+w)(u)=(\varphi(v)+\varphi(w))(u)=\varphi(v)(u)+\varphi(w)(u)=uv+uw.
	\]
	Además
	$(a\otimes b)(x\otimes y)=\varphi(x\otimes y)(a\otimes b)=(R_x\otimes R_y)(a\otimes b)=ax\otimes by$.
	Un cálculo sencillo muestra que este producto es asociativo.
\end{proof}

\begin{exercise}
	Demuestre que para álgebras valen las siguientes afirmaciones:
	\begin{enumerate}
		\item $A\otimes B\simeq B\otimes A$.
		\item $(A\otimes B)\otimes C\simeq A\otimes(B\otimes C)$.
		\item $A\otimes K\simeq A\simeq K\otimes A$.
		\item Si $A\otimes A'$ y $B\otimes B'$ entonces $A\otimes B\simeq A'\otimes B'$.
	\end{enumerate}
\end{exercise}

Veamos algunos ejemplos:

\begin{proposition}
	Si $G$ y $H$ son grupos, entonces $K[G]\otimes K[H]\simeq K[G\times H]$.
\end{proposition}

\begin{proof}
	Sabemos que $\{g\otimes h:g\in G,h\in H\}$ es una base de $K[G]\otimes K[H]$ y que
	$G\times H$ es una base de $K[G\times H]$. Tenemos entonces un isomorfismo lineal 
	\[
	K[G]\otimes K[H]\to K[G\times H], 
	\quad 
	g\otimes h\mapsto (g,h),
	\]
	que además es multiplicativo. Luego $K[G]\otimes K[H]\simeq K[G\times H]$
	como álgebras.
\end{proof}

\begin{proposition}
	Si $A$ es un álgebra, entonces $A\otimes K[X]\simeq A[X]$.	
\end{proposition}

\begin{proof}
	Todo elemento de $A\otimes K[X]$ se escribe unívocamente como una suma
	finita de la forma $\sum a_i\otimes X^i$. Un cálculo sencillo muestra que
	$A\otimes K[X]\mapsto A[X]$, $\sum a_i\otimes X^i\mapsto \sum a_iX^i$, es
	un isomorfismo de álgebras.
\end{proof}

\begin{exercise}
	Demuestre que si $A$ es un álgebra, $A\otimes M_n(K)\simeq M_n(A)$. En
	particular, $M_n(K)\otimes M_m(K)\simeq M_{nm}(K)$.
\end{exercise}

\index{Extensión de escalares}
Estos últimos dos ejemplos son casos particulares de una construcción
importante que involucra productos tensoriales y se conoce como
\textbf{extensión de escalares}.

\begin{theorem}
	Sea $A$ un álgebra sobre $K$ y sea $E$ una extensión de $K$. Entonces
	$A^E=E\otimes_KA$ es un álgebra sobre $E$ con respecto a la multiplicación
	por escalares dada por
	\[
		\lambda(\mu\otimes a)=(\lambda\mu)\otimes a,
	\]
	para $\lambda,\mu\in E$ y $a\in A$.
\end{theorem}

\begin{proof}
	Sea $\lambda\in E$. Como la función $E\times A\to E\otimes_KA$,
	$(\mu,a)\mapsto (\lambda\mu)\otimes a$, es $K$-bilineal, existe una
	transformación lineal $E\otimes_KA\to E\otimes_KA$, $\mu\otimes a\mapsto
	(\lambda\mu)\otimes a$. Queda bien definida entonces la multiplicación por
	escalares y además 
	\[
	\lambda(u+v)=\lambda u+\lambda v
	\]
	para $\lambda\in E$ y $u,v\in E\otimes_KA$. Un cálculo directo muestra que además 
	\[
	(\lambda+\mu)u=\lambda u+\mu u,
	\quad
	(\lambda\mu)u=\lambda(\mu u),
	\quad
	\lambda(uv)=(\lambda u)v=u(\lambda v)
	\]
	valen para todo $u,v\in E\otimes_KA$ y $\lambda,\mu\in E$.
\end{proof}

\begin{exercise}
	Demuestre que valen las siguientes afirmaciones:
	\begin{enumerate}
		\item $1\otimes A$ es una subálgebra de $A^E$ isomorfa a $A$.
		\item Si $\{a_i:i\in I\}$ es base de $A$, entonces $\{1\otimes a_i:i\in
			I\}$ es base de $A^E$.
	\end{enumerate}
\end{exercise}

\begin{exercise}
	Demuestre que si $G$ es un grupo y $K$ es un subcuerpo de $E$, entonces
	$E\otimes_K K[G]\simeq E[G]$.
\end{exercise}

Estamos en condiciones de demostrar el teorema de Formanek:

\begin{theorem}[Formanek]
	\index{Teorema!de Formanek}
	Sea $K$ un cuerpo de característica cero y sea $G$ un grupo. Si todo
	elemento de $K[G]$ es inversible o un divisor de cero, entonces $G$ es
	localmente finito.
\end{theorem}

\begin{proof}
	Como $K$ es de característica cero, $\Q\subseteq K$ y $K[G]\simeq
	K\otimes_{\Q}\Q[G]$. Todo $\beta\in K\otimes_{\Q}\Q[Q]$ se escribe
	unívocamente como 
	\[
		\beta=1\otimes\beta_0+\sum k_i\otimes\beta_i,
	\]
	donde $\{1,k_1,k_2,\dots,\}$ es una base de $K$ como $\Q$-espacio
	vectorial. Sea $\alpha\in\Q[G]$ y sea $\beta\in K[G]$ tal que $\alpha\beta=1$. Como entonces 
	\[
	1\otimes 1=(1\otimes\alpha)\beta=1\otimes \alpha\beta_0+\sum k_i\otimes \alpha\beta_i,
	\]
	la unicidad de la escritura nos dice que $\alpha\beta_0=1$. De la misma
	forma, si $\alpha\beta=0$, entonces $\alpha\beta_j=0$ para todo $j$. Luego,
	como todo $\alpha\in\Q[G]$ es inversible o un divisor de cero, el resultado
	se obtiene al usar el teorema~\ref{thm:FormanekQ} de Formanek para $\Q$.
\end{proof}

\section*{Rickart's theorem}

En esta sección vamos a demostrar que para cualquier grupo $G$ el radical de
Jacobson de $\C[G]$ es cero. Demostraremos también que el radical de Jacobson
de $\R[G]$ es cero.

\begin{definition}
	\index{Anillo!con involución}
	\index{Involución!de un anillo}
	Sea $R$ un anillo. Una \textbf{involución} del anillo $R$ es un morfismo
	aditivo $R\to R$, $x\mapsto x^*$, tal que $x^{**}=x$ y $(xy)^*=y^*x^*$ para
	todo $x,y\in R$.
\end{definition}

De la definición se deduce inmediatamente que si $R$ es unitario, entonces
$1^*=1$.

\begin{example}
	La conjugación $z\mapsto\overline{z}$ es una involución de $\C$.
\end{example}

\begin{example}
	La trasposición $X\mapsto X^T$ es una involución del
	anillo $M_n(K)$.
\end{example}

\begin{example}
	Sea $G$ un grupo. Entonces
	$\left(\sum_{g\in G}\alpha_gg\right)^*=\sum_{g\in G}\overline{\alpha_g}g^{-1}$ 
	es una involución de $\C[G]$.
\end{example}

Dado un grupo $G$, se define la \textbf{traza} de un elemento
$\alpha=\sum_{g\in G}\alpha_gg\in K[G]$ como $\trace(\alpha)=\alpha_1$. Es
fácil ver que $\trace\colon K[G]\to K$, $\alpha\mapsto\trace(\alpha)$ es una
función $K$-lineal tal que $\trace(\alpha\beta)=\trace(\beta\alpha)$.

\begin{exercise}
	Sea $G$ un grupo finito y $K$ un cuerpo tal que su característica no divide al orden de $G$.
	Demuestre las siguientes afirmaciones:
	\begin{enumerate}
		\item Si $\alpha\in K[G]$ es nilpotente, entonces $\trace(\alpha)=0$.
		\item Si $\alpha\in K[G]$ es idempotente, entonces $\trace(\alpha)=\dim
			K[G]\alpha/|G|$.
	\end{enumerate}
\end{exercise}

\begin{exercise}
	Demuestre que 
	$\langle\alpha,\beta\rangle=\trace(\alpha\beta^*)$, $\alpha,\beta\in\C[G]$, 
	define un producto interno en $\C[G]$.
\end{exercise}

\begin{lemma}
	\label{lem:algebraico}
	Sea $G$ un grupo. Si $J(\C[G])\ne 0$, entonces existe $\alpha\in J(\C[G])$ tal que 
	$\trace(\alpha^{2^m})\in\R_{\geq1}$ 
	para todo $m\geq1$.
\end{lemma}

\begin{proof}
	Sea $\alpha=\sum_{g\in G}\alpha_gg\in\C[G]$. Entonces	
	\[
		\trace(\alpha^*\alpha)
		=\sum_{g\in G}\overline{\alpha_g}\alpha_g
		=\sum_{g\in G}|\alpha_g|^2\geq|\alpha_1|^2
		=|\trace(\alpha)|^2.
	\]
	Al usar esta fórmula para algún $\alpha$ tal que $\alpha^*=\alpha$ y usar
	inducción se obtiene que $\trace(\alpha^{2^m})\geq|\trace(\alpha)|^{2^m}$
	para todo $m\geq1$. 

	Sea $\beta=\sum_{g\in G}\beta_gg\in J(\C[G])$ tal que $\beta\ne0$. Como
	$\trace(\beta^*\beta)=\sum_{g\in G}|\beta_g|^2\ne0$ y $J(\C[G])$ es un ideal, 
	\[
		\alpha=\frac{\beta^*\beta}{\trace(\beta^*\beta)}\in J(\C[G]).
	\]
	Este elemento $\alpha$ cumple que $\alpha^*=\alpha$ y $\trace(\alpha)=1$.
	Luego $\trace(\alpha^{2^m})\geq 1$ para todo $m\geq1$.
\end{proof}

El ejercicio~\ref{exa:norma} implica que $\C[G]$ con
$\dist(\alpha,\beta)=|\alpha-\beta|$ es un espacio métrico. En este espacio
métrico, la función $\C[G]\to\C$, $\alpha\mapsto \trace(\alpha)$, es una
función continua.

\begin{lemma}
	\label{lem:phi_diferenciable}
	Sea $\alpha\in J(\C[G])$. La función
	\[
		\varphi\colon\C\to\C[G],\quad
		\varphi(z)=(1-z\alpha)^{-1},
	\]
	es continua, diferenciable y $\varphi(z)=\sum_{n\geq0}\alpha^nz^n\in\C[G]$ si $|z|$
	es suficientemente pequeño.
\end{lemma}

\begin{proof}	
	Sean $y,z\in\C$. Como $\varphi(y)$ y $\varphi(z)$ conmutan, 
	\begin{equation}
		\label{eq:Rickart}
		\begin{aligned}
			\varphi(y)-\varphi(z)&=\left( (1-z\alpha)-(1-y\alpha)\right)(1-y\alpha)^{-1}(1-z\alpha)^{-1}\\
			&=(y-z)\alpha\varphi(y)\varphi(z).
		\end{aligned}
	\end{equation}
	Entonces $|\varphi(y)|\leq|\varphi(z)|+|y-z||\alpha\varphi(y)||\varphi(z)|$ y luego
	\[
		|\varphi(y)|\left( 1-|y-z||\alpha\varphi(z)|\right)\leq|\varphi(z)|.
	\]
	Fijado $z$ podemos elegir $y$ suficientemente cerca de $z$ de forma tal que
	se cumpla que  $1-|y-z||\alpha\varphi(z)|\geq1/2$. Luego
	$|\varphi(y)|\leq2|\varphi(z)|$. De la igualdad~\eqref{eq:Rickart} se
	obtiene entonces $|\varphi(y)-\varphi(z)|\leq2|y-z||\alpha||\varphi(z)|^2$
	y luego $\varphi$ es una función continua. Por la
	expresión~\eqref{eq:Rickart}, 
	\[
	\varphi'(z)
	=\lim_{y\to z}\frac{\varphi(y)-\varphi(z)}{y-z}
	=\lim_{y\to z}\alpha\varphi(y)\varphi(z)
	=\alpha\varphi(z)^2
	\]
	para todo $z\in\C$.

	Si $z$ es tal que $|z||\alpha|=|z\alpha|<1$, entonces 
	\[
		\varphi(z)-\sum_{n=0}^Nz^n\alpha^n
		=\varphi(z)\left(1-(1-z\alpha)\sum_{n=0}^Nz^n\alpha^n\right)
		=\varphi(z)(z\alpha)^{N+1}
	\]
	y luego
	\[
		\left|\varphi(z)-\sum_{n=0}^Nz^n\alpha^n\right|\leq|\varphi(z)||z\alpha|^{N+1}.
	\]
	Como $\varphi(z)$ está acotada cerca de $z=0$, se concluye que
	$\left|\varphi(z)-\sum_{n=0}^Nz^n\alpha^n\right|\to0$ si $N\to\infty$.
\end{proof}

Estamos en condiciones de demostrar el teorema de Rickart:

\begin{theorem}[Rickart]
	\index{Teorema!de Rickart}
	Si $G$ es un grupo, entonces $J(\C[G])=0$.
\end{theorem}

\begin{proof}
	Sea $\alpha\in J(\C[G])$ y sea $\varphi(z)=(1-\alpha z)^{-1}$. Sea 
	$f\colon\C\to \C$ dada por
	$f(z)=\trace\varphi(z)=\trace\left((1-z\alpha)^{-1}\right)$. Por el lema~\ref{lem:phi_diferenciable},
	$f(z)$ es una función entera tal que $f'(z)=\trace(\alpha\varphi(z)^2)$ y
	\begin{equation}
		\label{eq:Taylor}
		f(z)=\sum_{n=0}^\infty z^n\trace(\alpha^n)
	\end{equation}
	si $|z|$ es suficientemente pequeño. En particular, la
	igualdad~\eqref{eq:Taylor} es la expansión en serie de Taylor para $f(z)$
	en el origen. Esto implica que esta serie tiene radio de convergencia
	infinito y converge a $f(z)$ para todo $z\in\C$. En particular,
	\begin{equation}
		\label{eq:limite}
		\lim_{n\to\infty}\trace(\alpha^n)=0.
	\end{equation}
	Por otro lado, si $\alpha\ne0$ el lema~\ref{lem:algebraico} implica que
	$\trace(\alpha^{2^m})\geq1$ para todo $m\geq0$, lo que contradice el límite
	calculado en~\eqref{eq:limite}. Luego $\alpha=0$.
\end{proof}

Para demostrar un corolario necesitamos dos lemas:

\begin{lemma}[Nakayama]
	\label{lem:Nakayama}
	\index{Lema!de Nakayama}
	Sea $R$ un anillo unitario y sea $M$ un $R$-módulo finitamente generado. Si
	$J(R)M=M$, entonces $M=0$.
\end{lemma}

\begin{proof}
	Supongamos que $M$ está generado por los elementos $x_1,\dots,x_n$. Como $x_n\in M=J(R)M$, 
	existen $r_1,\dots,r_n\in J(R)$ tales que $x_n=r_1x_1+\cdots+r_nx_n$, es decir
	$(1-r_n)x_n=\sum_{j=1}^{n-1}r_jx_j$. 
	Como $1-r_n$ es inversible, existe $s\in R$ tal que $s(1-r_n)=1$. Luego
	$x_n=\sum_{j=1}^{n-1}sr_jx_j$ 
	y entonces $M$ está generado por $x_1,\dots,x_{n-1}$. Al repetir este
	procedimiento una cierta cantidad finita de veces, se obtiene que $M=0$.
\end{proof}

\begin{lemma}
	\label{lem:Rickart}
	Sea $\iota\colon R\to S$ un morfismo de anillos unitarios. Si 
	\[
	S=\iota(R)x_1+\cdots+\iota(R)x_n,
	\]
	donde cada $x_j$ cumple que $x_jy=yx_j$ para todo $y\in\iota(R)$, entonces
	$\iota(J(R))\subseteq J(S)$.
\end{lemma}

\begin{proof}
	Veamos que $J=\iota(J(R))$ actúa trivialmente en cada $S$-módulo simple $M$.
	Si $M$ es un $S$-módulo simple, escribimos $M=Sm$ para algún $m\ne0$. Es
	claro que $M$ es un $R$-módulo con $r\cdot m=\iota(r)m$. Como
	\[
		M=Sm=(\iota(R)x_1+\cdots+\iota(R)x_n)m=\iota(R)(x_1m)+\cdots+\iota(R)(x_nm),
	\]
	$M$ es finitamente generado como $\iota(R)$-módulo. Además $J(R)\cdot
	M=JM=\iota(J)M$ es un $S$-submódulo de $M$ pues
	\[
		x_j(JM)=(x_jJ)M=(Jx_j)M=J(x_jM)\subseteq JM.
	\]
	Como $M\ne0$, el lema de Nakayama implica que $J(R)\cdot M\subsetneq M$. Luego,
	como $M$ es un $S$-módulo simple, se concluye que $J(R)M=0$.
\end{proof}

\begin{corollary}
	Si $G$ es un grupo, entonces $J(\R[G])=0$. 
\end{corollary}

\begin{proof}
	Sea $\iota\colon \R[G]\to\C[G]$ la inclusión canónica. Como 
	\[
	\C[G]=\R[G]+i\R[G],
	\]
	el lema~\ref{lem:Rickart} y el teorema de Rickart implican que
	$\iota(J(\R[G]))\subseteq J(\C[G])=0$. Luego $J(\R[G])=0$ pues $\iota$ es
	inyectiva. 
\end{proof}
