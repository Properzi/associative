\chapter{}

\section*{Maschke's theorem}

Sea $K$ un cuerpo y sea $G$ un grupo. El \textbf{álgebra de grupo} $K[G]$ es el
$K$-espacio vectorial con base $\{g:g\in G\}$ con la estructura de álgebra dada
por el producto
\[
	\left(\sum_{g\in G}\lambda_gg\right)\left(\sum_{h\in G}\mu_hh\right)
	=\sum_{g,h\in G}\lambda_g\mu_h(gh).
\]

\index{Ideal!de aumentación}
Es fácil ver que si $G$ no es el grupo trivial, entonces $K[G]$ nunca es un álgebra simple. Esto es porque el conjunto 
\[
	I(G)=\left\{\sum_{g\in G}\lambda_gg\in K[G]:\sum_{g\in G}\lambda_g=0\right\}
\]
es un ideal propio y no nulo de $K[G]$ (pues $\dim I(G)=\dim K[G]-1$). Este
conjunto se conoce como el \textbf{ideal de aumentación} de $K[G]$.

\begin{exercise}
	Sea $G=C_n$ el grupo ciclico de orden $n$ (escrito multiplicativamente).
	Demuestre que $K[G]\simeq K[X]/(X^n-1)$. 
\end{exercise}

\begin{exercise}
	Sea $G$ un grupo finitamente generado abeliano y sin torsión. Demuestre que
	$K[G]$ es un dominio.
\end{exercise}

\begin{exercise}
	Sea $G$ un grupo y $H$ un subgrupo de $G$. Sea $\alpha\in K[H]$. Demuestre
	que $\alpha$ es inversible (resp. divisor de cero a izquierda) en $K[H]$ si
	y sólo si $\alpha$ es inversible (resp. divisor de cero a izquierda) en
	$K[G]$.
\end{exercise}

\begin{exercise}
	\index{Soporte}
	Sean $G$ un grupo y $\alpha=\sum_{g\in G}\lambda_gg\in K[G]$.  
	Se define el \textbf{soporte} de $\alpha$ como el conjunto
	\[
		\supp\alpha=\{g\in G:\lambda_g\ne 0\}.
	\]
	Demuestre que si $g\in G$, entonces
	$\supp(g\alpha)=g(\supp\alpha)$ y $\supp(\alpha g)=(\supp\alpha)g$.
\end{exercise}

% El objetivo de esta sección es calcular el radical de Jacobson del álgebra de
% grupo de un grupo finito. Comenzamos con un ejemplo:

\begin{exercise}
	Sea $G=C_2=\langle g\rangle\simeq\Z/2$ el grupo de dos elementos escrito
	multiplicativamente. Todo elemento de $K[G]$ es de la forma
	$\alpha=a1+bg$ para escalares $a,b\in K$ y el producto
	de $K[G]$ está dado por
	\[
		(a1+bg)(c1+dg)=(ac+bd)1+(ad+bc)g.
	\]
	Si la
	característica de $K$ es distinta de dos,  
	la función \[
		K[G]\to K\times K,
		\quad
		a1+bg\mapsto (a+b,a-b),
	\]
	es un isomorfismo de álgebras.  Si en cambio $K$ es de característica dos,
	la función
	\[
		K[G]\to \begin{pmatrix}
			K & K\\
			0 & K
		\end{pmatrix},
		\quad
		a1+bg\mapsto\begin{pmatrix}
			a+b & b\\
			0 & a+b
		\end{pmatrix},
	\]
	es un isomorfismo de álgebras.
\end{exercise}

Veamos otros ejemplo un poco más difíciles. La idea a utilizar es la siguiente:
Si $A$ es una $K$-álgebra y $\rho\colon G\to U(A)$ es un morfismo de grupos,
donde $U(A)$ es el grupo de unidades de $A$, entonces la función $K[G]\to A$,
$\sum_{g\in G}\lambda_gg\mapsto\sum_{g\in G}\lambda_g\rho(g)$, es un morfismo
de álgebras.


\begin{example}
	Sea $G=C_3$ el grupo cíclico de orden tres (escrito multiplicativamente).
	Entonces 
	\[
		\R[G]\simeq\R\times\C.
	\]
	Escribamos $G=\langle g:g^3=1\rangle$ y sea 
	\[
		\varphi\colon\R[G]\to\R\times\C,
		\quad
		g\mapsto (1,\omega),
	\]
	donde $\omega$ es una raíz cúbica primitiva de la unidad. Entonces
	$\varphi$ es inyectivo pues
	$0=\varphi(a1+bg+cg^2)=(a+b+c,a+b\omega+c\omega^2)$ implica que $a=b=c=0$.
	Luego $\varphi$ es un isomorfismo pues
	$\dim_\R\R[G]=\dim_\R(\R\times\C)=3$. 
\end{example}

\begin{example}
	Sea $G=\langle r,s:r^3=s^2=1,\,srs=r^{-1}\rangle$ el grupo diedral de seis
	elementos. Vamos a demostrar que $\C[G]\simeq\C\times\C\times M_2(\C)$.  
	Sea $\omega$ una raíz cúbica de la unidad y sean  
	\[
		R=\begin{pmatrix}
			\omega & 0\\
			0 & \omega^2
		\end{pmatrix},
		\quad
		S=\begin{pmatrix}
			0 & 1\\
			1 & 0
		\end{pmatrix}.
	\]
	Un cálculo sencillo muestra que $R^2=S^2=I$ y que $SRS=R^{-1}$. Sea
	\[
		\varphi\colon\C[G]\to\C\times\C\times M_2(\C),\quad
		r\mapsto (1,1,R),\quad
		s\mapsto (1,-1,S).
	\]
	Es fácil ver que $\varphi$ es un morfismo de álgebras. Veamos que es
	biyectivo. Como $\dim_{\C}\C[G]=\dim_{\C}(\C\times\C\times M_2(\C))=6$,
	basta ver que $\varphi$ es inyectivo. Si 
	\[
		\alpha=a_0+a_1r+a_2r^2+(b_0+b_1r+b_2r^2)s\in\ker\varphi,
	\]
	entonces 
	\[
		0=\varphi(\alpha)=\left(u,v,\begin{pmatrix} \alpha_{11} & \alpha_{12}\\\alpha_{21}&\alpha_{22}\end{pmatrix}\right), 
	\]
	donde
	\begin{align*}
		&u = a_0+a_1+a_2+b_0+b_1+b_2, && v = a_0+a_1+a_2-b_0-b_1-b_2,\\
		&\alpha_{11}=a_0+a_1\omega+a_2\omega^2, && \alpha_{12}=b_0+b_1\omega+b_2\omega^2,\\
		&\alpha_{21}=b_0+b_2\omega+b_1\omega^2, && \alpha_{22}=a_0+a_2\omega+a_1\omega^2.
	\end{align*}
	Un cálculo sencillo muestra que estas ecuaciones implican que
	$\alpha=0$ y luego $\varphi$ es inyectiva.  
\end{example}

\begin{exercise}
	Demuestre que si $G$ es el grupo diedral de seis elementos entonces
	\[
		\Q[G]\simeq\Q\times\Q\times M_2(\Q),
	\]
	donde $\omega$ es una raíz cúbica primitiva de la unidad.
\end{exercise}

\begin{theorem}[Maschke]
	\index{Teorema!de Maschke}
%	\label{thm:Maschke}
	Sea $G$ un grupo finito. Entonces $J(K[G])=0$ si y sólo si $K$ es de
	característica cero o la característica de $K$ no divide al orden de $G$.
\end{theorem}

\begin{proof}
	Supongamos que $G=\{g_1,\dots,g_n\}$ con $g_1=1$. Sea $\rho\colon K[G]\to
	K$ dada por $\alpha\mapsto\trace(L_{\alpha})$, donde
	$L_{\alpha}(\beta)=\alpha\beta$. Tenemos $\rho(g_1)=n$ y $\rho(g_i)=0$ para
	todo $i\in\{2,\dots,n\}$ pues,  como $L_{g_i}(g_j)=g_{i}g_j\ne g_j$, la
	matriz de $L_{g_i}$ en la base $\{g_1,\dots,g_n\}$ tiene ceros en la
	diagonal.

	Supongamos que $J=J(K[G])$ es no nulo y sea
	$\alpha=\sum_{i=1}^n\lambda_ig_i\in J\setminus\{0\}$. Sin pérdida de
	generalidad podemos suponer que $\lambda_1\ne 0$ (pues si $\lambda_1=0$ hay
	algún $\lambda_i\ne 0$ y alcanza con tomar $g_i^{-1}\alpha\in J$). Entonces
	\[
		\rho(\alpha)=\sum_{i=1}^n \lambda_i\rho(g_i)=n\lambda_1.
	\]
	Como $G$ es un grupo finito, $K[G]$ es un álgebra de dimensión finita y
	luego $K[G]$ es artiniana a izquierda. Como el radical de Jacobson $J$ es
	un ideal nilpotente, en particular $\alpha$ es un elemento nil. Luego
	$L_{\alpha}$ es nilpotente y entonces $0=\rho(\alpha)=n\lambda_1$. Esto
	implica que la característica del cuerpo $K$ divide a $n$. 

	Recíprocamente, supongamos que la característica de $K$ es un número primo
	que divide a $n$ y sea $\alpha=\sum_{i=1}^ng_i$. Como $\alpha
	g_j=g_j\alpha=\alpha$ para todo $j\in\{1,\dots,n\}$, el conjunto
	$I=K[G]\alpha$ es un ideal de $K[G]$. Como además 
	\[
		\alpha^2=\sum_{i=1}^n g_i\alpha=n\alpha=0,
	\]
	se concluye que $I$ es un ideal no nulo y nilpotente. Luego $J(K[G])\ne 0$
	pues por la proposición~\ref{pro:nilJ} sabemos que $I\subseteq J(K[G])$.
\end{proof}

\begin{corollary}
	\label{cor:GfinitoNOnil}
	Sea $G$ un grupo finito. Entonces $K[G]$ no contiene ideales a izquierda
	nil no nulos.
\end{corollary}

\begin{proof}
	Es consecuencia inmediata del teorema de Maschke ya que $J(K[G])$ contiene a
	todo ideal a izquierda nil.	
\end{proof}

%\index{Anillo!semisimple}
%Recordemos que un anillo unitario $R$ se dice \textbf{semisimple} si para cada
%ideal $I$ de $R$ existe un ideal $J$ de $R$ tal que $R=I\oplus J$.
%
%%\begin{corollary}
%%	Sea $G$ un grupo finito y $K$ un cuerpo de característica coprima con el
%%	orden de $G$. Entonces $K[G]$ es semisimple.
%%\end{corollary}
%%
%%\begin{proof}
%%	
%%\end{proof}
%
%\begin{theorem}
%	Si $G$ es un grupo infinito, entonces $K[G]$ nunca es semisimple.
%\end{theorem}
%
%\begin{proof}
%	Sea $R=K[G]$ y supongamos que $R$ es semisimple.  Si $I$ es el ideal de
%	aumentación de $R$, existe un ideal no nulo $J$ de $R$ tal que $R=I\oplus
%	J$. Como $R$ es unitario, existen $e\in I$, $f\in J$ tales que $1=e+f$. Si
%	$x\in I$, entonces $x=xe+xf$ y luego $xf=x-xe\in I\cap J=\{0\}$. Como
%	entonces $x=xe$ para todo $x\in I$, en particular $e_1=e_1^2$. Análogamente
%	vemos que $e_2^2=e_2$. Además $ef=0$ pues $ef\in I\cap J=\{0\}$.  Como $I$
%	es el ideal de aumentación y $If=(Re)f=R(ef)=0$, se concluye que $(g-1)f=0$
%	para todo $g\in G$ pues $g-1\in I$. Si suponemos que $f=\sum_{h\in
%	G}\lambda_hh$, entonces 
%	\[
%	f=gf=\sum_{h\in G}\lambda_h(gh)=\sum_{h\in
%	G}\lambda_{g^{-1}h}h.
%	\]
%	Luego $\lambda_h=\lambda_{g^{-1}h}$ para todo $g,h\in G$, una contradicción
%	pues como $f\ne 0$ la suma que define a $f$ es infinita. 
%\end{proof}


\section*{Herstein's theorem}

El objetivo de esta sección responderemos la siguiente pregunta: ¿Cuándo un
álgebra de grupo es un álgebra algebraica? Una respuesta parcial está dada por
el teorema de Herstein. 

\begin{definition}
	\index{Grupo!localmente finito}
	Un grupo $G$ se dice \textbf{localmente finito} si todo subgrupo de $G$
	finitamente generado es finito.
\end{definition}

Si $G$ es un grupo localmente finito, entonces todo $g\in G$ tiene orden finito
(pues el subgrupo $\langle g\rangle$ es finito por ser finitamente generado). 

\begin{example}
	Todo grupo finito es obviamente localmente finito.
\end{example}

\begin{example}
	El grupo $\Z$ no es localmente finito pues es libre de torsión.
\end{example}

\begin{example}
	\index{Grupo!de Pr\"ufer}
	Sea $p$ un primo.  El \textbf{grupo de Pr\"ufer}
	\[
		\Z(p^{\infty})=\{z\in\Z:z^{p^n}=1\text{ para algún $n\in\N$}\}
	\]
	de todas
	las raíces $p$-ésimas de uno es localmente finito.
\end{example}

\begin{example}
	Sean $X$ un conjunto infinito y $\Sym_X$ el conjunto de biyecciones $X\to
	X$ que mueven únicamente una cantidad finita de elementos de $X$. Entonces
	$\Sym_X$ es localmente finito.
\end{example}

Antes de demostrar el teorema de Herstein vamos a dar una familia de ejemplos
de grupos localmente finitos. Para eso necesitamos un lema:

\begin{lemma}
	\label{lem:solvable_torsion=>lf}
	Sea $G$ un grupo y sea $N$ un subgrupo normal de $G$. Si $N$ y $G/N$ son
	localmente finitos, entonces $G$ es localmente finito.
\end{lemma}

\begin{proof}
	Sea $\pi\colon G\to G/N$ el morfismo canónico. Sea $\{g_1,\dots,g_n\}$ un
	subconjunto finito de $G$. Como $G/N$ es localmente finito, el subgrupo $Q$
	de $G/N$ generado por $\pi(g_1),\dots,\pi(g_n)$ es finito, digamos
	\[
		Q=\{\pi(g_1),\dots,\pi(g_n),\pi(g_{n+1}),\dots,\pi(g_m)\}.
	\]
	Para cada $i,j\in\{1,\dots,n\}$ sabemos que existen $u_{ij}\in N$ y
	$k\in\{1,\dots,m\}$ tales que $g_ig_j=u_{ij}g_k$. Sea $U$ el subgrupo de
	$G$ generado por los $u_{ij}$. Como $N$ es localmente finito, $U$ es un
	subgrupo finito. Como además cada elemento $g_ig_jg_l$ puede escribirse como
	\[
		g_ig_jg_l=u_{ij}g_kg_l=u_{ij}u_{kl}g_t=ug_t
	\]
	para algún $u\in U$ y algún $t\in\{1,\dots,m\}$, se concluye que el
	subgrupo $H$ de $G$ generado por $\{g_1,\dots,g_n\}$ es finito pues
	$|H|\leq m|U|$. 
\end{proof}

\index{Grupo!resoluble}
Veamos una aplicación a los grupos resolubles. Recordemos que un grupo $G$ se
dice \textbf{resoluble} si existe una sucesión de subgrupos 
\begin{equation}
	\label{eq:resoluble}
	1=G_0\subsetneq G_1\subsetneq \cdots\subsetneq G_n=G
\end{equation}
donde cada $G_i$ es normal en $G_{i+1}$ y cada cociente $G_i/G_{i-1}$ es
abeliano.

\begin{proposition}
	Si $G$ es un grupo resoluble y de torsión, entonces $G$ es localmente
	finito.
\end{proposition}

\begin{proof}
	Procederemos por inducción en la longitud $n$ de la sucesión de
	resolubilidad~\eqref{eq:resoluble}. Si $n=1$ entonces $G$ es finito por ser
	abeliano y de torsión. Supongamos que el resultado vale para grupos
	resolubles de longitud $n-1$ y sea $G$ un grupo resoluble tal
	que~\eqref{eq:resoluble}. Por hipótesis inductiva, el subgrupo normal
	$G_{n-1}$ de $G$ es localmente finito. Entonces, como $G/G_{n-1}$ es
	localmente finito por ser abeliano y de torsión, el resultado se obtiene
	del lema~\ref{lem:solvable_torsion=>lf}.
\end{proof}

\begin{theorem}[Herstein]
	\index{Teorema!de Herstein}
	Si $G$ es un grupo localmente finito, entonces $K[G]$ es algebraica.
	Recíprocamente, si $K[G]$ es algebraica y $K$ es de característica cero,
	entonces $G$ es localmente finito.
\end{theorem}

\begin{proof}
	Supongamos que $G$ es localmente finito y sea $\alpha\in K[G]$. El subgrupo
	$H=\langle\supp\alpha\rangle$ es finitamente generado y luego finito. Como
	$\alpha\in K[H]$ y $\dim_KK[H]<\infty$, el conjunto
	$\{1,\alpha,\alpha^2,\dots\}$ es linealmente dependiente. Luego $\alpha$ es
	algebraico sobre $K$.

	Sea $\{x_1,\dots,x_m\}$ un subconjunto finito de $G$. Si agregamos los
	inversos, podemos suponer que $\{x_1,\dots,x_m\}$ genera al subgrupo
	$H=\langle x_1,\dots,x_m\rangle$ como semigrupo. Si
	$\alpha=x_1+\dots+x_m\in K[G]$, entonces, como $\alpha$ es algebraico sobre
	$K$, 
	\[
		\alpha^{n+1}=a_0+a_1\alpha+\cdots+a_n\alpha^n
	\]
	para algún $n\geq0$ y escalares $a_0,\dots,x_n\in K$. Sea $w=x_{i_1}\cdots
	x_{i_{n+1}}\in H$ una palabra de longitud $n+1$. Observemos que existen enteros
	positivos $c_{i_1\cdots i_m}$ tales que 
	\[
		\alpha^{n+1}=(x_1+\cdots+x_m)^{n+1}
		=\sum_{\substack{{i_1+\cdots+i_m=n+1}\\{\text{$i_j$ enteros positivos}}}} c_{i_1\cdots i_m}x_1^{i_1}\cdots x_{m}^{i_m}.
	\]
	Como $K$
	es de característica cero, se concluye que $w\in\supp(\alpha^{n+1})$.  Pero
	como además $\alpha^{n+1}=\sum_{j=0}^na_j\alpha^j$, entonces
	$w\in\supp(\alpha^j)$ para algún $j\in\{0,\dots,n\}$. Demostramos entonces
	que toda palabra en las $x_j$ de longitud $n+1$ puede escribirse como una
	palabra en las $x_j$ de longitud a lo sumo $n$.  Luego $H$ es finito y
	entonces $G$ es localmente finito.
\end{proof}

\section*{Formanek's theorem}

Veremos un resultado de Formanek que puede entenderse como una generalización
del teorema de Herstein. 

\begin{exercise}
	Sea $A$ un álgebra algebraica y sea $a\in A$. Demuestre las siguientes
	afirmaciones:
	\begin{enumerate}
		\item $a$ es un divisor de cero a izquierda si y sólo si $a$ es un
			divisor de cero a derecha.
		\item $a$ es inversible a izquierda si y sólo si $a$ es inversible a
			derecha.
		\item $a$ es inversible si y sólo si $a$ no es un divisor de cero.
	\end{enumerate}
\end{exercise}

%\begin{proof}
%	Como $a$ es algebraico, podemos escribir 
%	\[
%		a^n(1+\lambda_1a+\cdots+\lambda_ma^m)=0
%	\]
%	para algún $n\geq0$ minimal y escalares $\lambda_1,\dots,\lambda_m$. Si 
%	$n>0$, entonces 
%	\[
%	b=(1+\lambda_1a+\cdots+\lambda_ma^m)a^{n-1}\ne 0
%	\]
%	cumple que $ab=ba=0$. Si $n=0$, entonces 
%	\[
%		c=-\lambda_1-\lambda_2a-\cdots-\lambda_ma^{m-1}\ne 0
%	\]
%	cumple que $ac=ca=1$. 
%\end{proof}

\begin{exercise}
	\label{exa:norma}
	Si $\alpha=\sum_{g\in G}\alpha_gg\in\C[G]$ se define $|\alpha|=\sum_{g\in
	G}|\alpha_g|\in\R$. Demuestre que valen las siguientes propiedades:
	\begin{enumerate}
		%\item $|\trace(\alpha)|\leq |\alpha|$, 
		\item $|\alpha+\beta|\leq|\alpha|+|\beta|$, y 
		\item $|\alpha\beta|\leq|\alpha||\beta|$ 
	\end{enumerate}
	para todo $\alpha,\beta\in\C[G]$.
\end{exercise}

\begin{theorem}[Formanek, primera versión]
	\label{thm:FormanekQ}
	\index{Teorema!de Formanek}
	Sea $G$ un grupo y supongamos que todo elemento de $\Q[G]$ es inversible o
	un divisor de cero. Entonces $G$ es localmente finito.
\end{theorem}

\begin{proof}
	Sea $\{x_1,\dots,x_n\}$ un subconjunto finito de $G$. Si agregamos los
	inversos, podemos suponer que $\{x_1,\dots,x_n\}$ genera al subgrupo
	$H=\langle x_1,\dots,x_n\rangle$ como semigrupo. Sea
	\[
		\alpha=\frac{1}{2n}(x_1+\cdots+x_n)\in\Q[G]
	\]

	Veamos que $1-\alpha\in\Q[G]$ es inversible. Si no, entonces es un divisor de cero. 
	Si existe $\delta\in\Q[G]$ tal que $\delta(1-\alpha)=0$, entonces
	$\delta=\delta\alpha$ y luego, como 
	\[
		|\delta|=|\delta\alpha|\leq|\delta||\alpha|=|\delta|/2,
	\]
	se concluye que $\delta=0$. Similarmente se demuestra que $(1-\alpha)\delta=0$ implica que
	$\delta=0$. 
	
	Sea $\beta=(1-\alpha)^{-1}\in\Q[G]$.  Para cada $k$ definimos 
	\[
		\gamma_k=(1+\alpha+\cdots+\alpha^k)-\beta.
	\]
	Entonces 
	\begin{align*}
		\gamma_k(1-\alpha)&=(1+\alpha+\cdots+\alpha^k-\beta)(1-\alpha)\\
		&=(1+\alpha+\cdots+\alpha^k)(1-\alpha)-\beta(1-\alpha)=-\alpha^{k+1}
	\end{align*}
	y luego 
	$\gamma_k=-\alpha^{k+1}\beta$. Como 
	\[
		|\gamma_k|=|-\alpha^{k+1}\beta|\leq|\beta||\alpha^{k+1}|=\frac{|\beta|}{2^{k+1}},
	\]
	se concluye que $\lim_{k\to\infty}|\gamma_k|=0$. 

	Para terminar veamos que $H\subseteq\supp\beta$. Si
	$H\not\subseteq\supp\beta$, sea $h\in H\setminus\supp\beta$.  Supongamos
	que $h=x_{i_1}\cdots x_{i_m}$ es una palabra de longitud $m$ en los $x_j$.
	Sea $c_j$ el coeficiente de $h$ en $\alpha^j$. Entonces $c_0+\cdots+c_k$ es
	el coeficiente de $h$ en $\gamma_k$, pero
	\[
		|\gamma_k|\geq c_0+c_1+\cdots+c_k\geq c_m>0
	\]
	para todo $k\geq m$ pues cada $c_j$ es no negativo, una contradicción pues
	demostramos que $|\gamma_k|\to 0$ si $k\to\infty$.
\end{proof}

A continuación explicaremos por qué el teorema de Formanek se considera una
generalización del teorema de Herstein. En el teorema~\ref{thm:FormanekQ} nos
concentramos en álgebras de grupo sobre los números racionales. ¿Cómo podemos
extender este resultado a álgebras de grupo sobre cuerpos de característica
cero? Para extender el cuerpo de base sobre el que se trabaja necesitamos
definir el producto tensorial de espacios vectoriales y el producto tensorial
de álgebras.

\begin{definition}
	\index{Producto tensorial!de espacios vectoriales}
	El \textbf{producto tensorial} de los $K$-espacios vectoriales $U$ y $V$ es
	el espacio vectorial cociente $K[U\times V]/T$, donde $K[U\times V]$ es el
	espacio vectorial con base $\{(u,v):u\in U,v\in V\}$ y $T$ es el subespacio
	generado por los elementos de la forma
	\[
		(\lambda u+\mu u',v)-\lambda(u,v)-\mu(u',v),\quad
		(u,\lambda v+\mu v')-\lambda(u,v)-\mu(u,v')
	\]
	para $\lambda,\mu\in K$, $u,u'\in U$ y $v,v'\in V$.
\end{definition}

El producto tensorial de $U$ y $V$ será denotado por $U\otimes_KV$ o por
$U\otimes V$ si la referencia al cuerpo $K$ puede omitirse. Dados $u\in U$
y $v\in V$ escribiremos $u\otimes v$ para denotar a la coclase $(u,v)+T$.

\begin{theorem}
\index{Producto tensorial!propiedad universal}
	Sean $U$ y $V$ espacios vectoriales.  Existe entonces una función bilineal
	$U\times V\to U\otimes V$, $(u,v)\mapsto u\otimes v$, tal que todo
	elemento de $U\otimes V$ es una suma finita de la forma
	\[
		\sum_{i=1}^N u_i\otimes v_i
	\]
	para $u_1,\dots,u_N\in U$ y $v_1,\dots,v_N\in V$. 
	Más aún, dado un espacio vectorial $W$ y una función
	bilineal $\beta\colon U\times V\to W$, existe una función lineal
	$\overline{\beta}\colon U\otimes V\to W$ tal que $\overline{\beta}(u\otimes
	v)=\beta(u,v)$ para todo $u\in U$ y $v\in V$.
\end{theorem}

\begin{proof}
	Por la definición del producto tensorial, la función 
	\[
	U\times V\to U\otimes V,\quad
	(u,v)\mapsto u\otimes v,
	\]
	es bilineal. También de la definición se deduce inmediatamente que todo
	elemento de $U\otimes V$ es una combinación lineal finita de elementos de
	la forma $u\otimes v$, donde $u\in U$ y $v\in V$. Como $\lambda(u\otimes
	v)=(\lambda u)\otimes v$ para todo $\lambda\in K$, la primera afirmación
	queda demostrada.

	Como $U\times V$ es base de $K[U\times V]$, existe una transformación lineal 
	\[
		\gamma\colon K[U\times V]\to W,\quad
	\gamma(u,v)=\beta(u,v). 
	\]
	Como $\beta$ es bilineal por hipótesis, $T\subseteq\ker\gamma$. Existe
	entonces una transformación lineal $\overline{\beta}\colon U\otimes V\to
	W$ tal que 
	\[
	\begin{tikzcd}
		K[U\times V] \arrow[r]\arrow[d] & W \\
		U\otimes V\arrow[ur, dashrightarrow]
	\end{tikzcd}
	\]
	conmuta. En particular, $\overline{\beta}(u\otimes v)=\beta(u,v)$. 
\end{proof}

\begin{exercise}
	\label{xca:tensorial_unicidad}
	Demuestre que las propiedades mencionadas en el teorema anterior
	caracterizan el producto tensorial salvo isomorfismo.
\end{exercise}

Veamos algunas propiedades del producto tensorial de espacios vectoriales. 
%Observemos
%que todo elemento de $U\otimes V$ es una suma finita
%de la forma 
%\[
%	\sum_{i=1}^N u_i\otimes v_i
%\]
%para $N\in\N$, $u_i\in U$ y $v_i\in V$. Esta expresión no es única. Vale además
%que $u\otimes 0=0=0\otimes v$ para todo $u\in U$ y $v\in V$.

\begin{lemma}
	\index{Producto tensorial!de transformaciones lineales}
	Sean $\varphi\colon U\to U'$ y $\psi\colon V\to V'$ transformaciones
	lineales. Existe entonces una única transformación lineal
	$\varphi\otimes\psi\colon U\otimes V\to U'\otimes V'$ tal que
	\[
		(\varphi\otimes\psi)(u\otimes v)=\varphi(u)\otimes\psi(v)
	\]
	para todo $u\in U$ y $v\in V$.
\end{lemma}

\begin{proof}
	Como la función $U\times V\to U\otimes V$,
	$(u,v)\mapsto\varphi(u)\otimes\psi(v)$, es bilineal, existe una
	transformación lineal $U\otimes V\to U\otimes V$, $u\otimes
	v\to\varphi(u)\otimes\psi(v)$. Luego la función
	\[
		\sum u_i\otimes v_i\mapsto\sum\varphi(u_i)\otimes\psi(v_i)
	\]
	está bien definida. 
\end{proof}

\begin{exercise}
	Demuestre las siguientes afirmaciones:
	\begin{enumerate}
		\item $(\varphi\otimes\psi)(\varphi'\otimes\psi')=(\varphi\varphi')\otimes(\psi\psi')$.
		\item Si $\varphi$ y $\psi$ son isomorfismos, entonces
			$\varphi\otimes\psi$ es un isomorfismo. 
		\item $(\lambda\varphi+\lambda'\varphi')\otimes\psi=\lambda\varphi\otimes\psi+\lambda'\varphi'\otimes\psi$.
		\item $\varphi\otimes(\lambda\psi+\lambda'\psi')=\lambda\varphi\otimes\psi+\lambda'\varphi\otimes\psi'$.
		\item Si $U\simeq U'$ y $V\simeq V'$, entonces $U\otimes V\simeq U'\otimes V'$.
	\end{enumerate}
\end{exercise}

\begin{lemma}
	Si $U$ y $V$ son espacios vectoriales, entonces 
	$U\otimes V\simeq V\otimes U$.
\end{lemma}

\begin{proof}
	Como la función $U\times V\to V\otimes U$, $(u,v)\mapsto v\otimes u$,
	existe una transformación lineal $U\otimes V\to V\otimes U$, $u\otimes
	v\mapsto v\otimes u$. Similarmente se demuestra que existe una
	transformación lineal $V\otimes U\to U\otimes V$, $v\otimes u\mapsto
	u\otimes v$. Luego $U\otimes V\simeq V\otimes U$.
\end{proof}

\begin{exercise}
	\label{xca:UxVxW}
	Demuestre que $(U\otimes V)\otimes W\simeq U\otimes(V\otimes W)$.
\end{exercise}

\begin{exercise}
	\label{xca:UxK}
	Demuestre que $U\otimes K\simeq K\simeq K\otimes U$.
\end{exercise}

\begin{lemma}
	\label{lem:U_LI}
	Sea $\{u_1,\dots,u_n\}\subseteq U$ un conjunto linealmente independiente y
	sean $v_1,\dots,v_n\in V$ tales que $\sum_{i=1}^n u_i\otimes v_i=0$.
	Entonces $v_i=0$ para todo $i\in\{1,\dots,n\}$.
\end{lemma}

\begin{proof}
	Sea $i\in\{1,\dots,n\}$ y sea $f_i\colon U\to K$, $f_i(u_j)=\delta_{ij}$.
	Como la función $U\times V\to V$, $(u,v)\mapsto f_i(u)v$, es bilineal, existe una función
	$\alpha_i\colon U\otimes V\to V$ lineal tal que $\alpha_i(u\otimes
	v)=f_i(u)v$. Luego
	\[
		v_i=\sum_{j=1}^n\alpha_i(u_j\otimes v_j)=\alpha_i\left(\sum_{j=1}^nu_j\otimes v_j\right)=0.
	\]
\end{proof}

\begin{exercise}
	\label{xca:uxv=0}
	Demuestre que si $u\otimes v=0$ y $v\ne 0$, entonces $u=0$.
\end{exercise}

\begin{theorem}
	Si $\{u_i:i\in I\}$ es una base de $U$ y $\{v_j:j\in J\}$ es una base de
	$V$, entonces $\{u_i\otimes v_j:i\in I,j\in J\}$ es una base de $U\otimes
	V$.
\end{theorem}

\begin{proof}
	Los $u_i\otimes v_j$ forman un conjunto de generadores pues  
	si $u=\sum_i\lambda_iu_i$ y $v=\sum_j\mu_jv_j$, entonces
	$u\otimes v=\sum_{i,j}\lambda_i\mu_ju_i\otimes v_j$. 
	Veamos ahora que los $u_i\otimes v_j$ son linealmente independientes. Para
	eso, queremos ver que cualquier subconjunto finito de los $u_i\otimes v_j$
	es linealmente independiente. Si $\sum_k\sum_l\lambda_{kl}u_{i_k}\otimes
	v_{j_l}=0$, entonces
	$0=\sum_{k}u_{i_k}\otimes\left(\sum_{l}\lambda_{kl}v_{j_l}\right)$ y luego,
	como los $u_{i_k}$ son linealmente indepentientes, el lema~\ref{lem:U_LI}
	implica que $\sum_{l}\lambda_{kl}v_{j_l}=0$. Luego $\lambda_{kl}=0$ para
	todo $k,l$ pues los $v_{j_l}$ son linealmente independientes.
\end{proof}

El teorema anterior implica inmediatamente que si $U$ y $V$ son espacios
vectoriales de dimensión finita entonces
\[
	\dim(U\otimes V)=(\dim U)(\dim V).
\]

\begin{corollary}
	Si $\{u_i:i\in I\}$ es base de $U$, entonces todo elemento de $U\otimes V$
	se escribe unívocamente como una suma finita $\sum_{i}u_i\otimes v_i$.
\end{corollary}

\begin{proof}
	Sabemos que todo elemento de $U\otimes V$ es una suma finita
	$\sum_i x_i\otimes y_i$, donde $x_i\in U$ y $y_i\in V$. Si escribimos 
	$x_i=\sum_j\lambda_{ij}u_j$, entonces
	\[
		\sum_i x_i\otimes y_i=\sum_i\left(\sum_j\lambda_{ij}u_j\right)\otimes y_i		
		=\sum_j u_j\otimes\left(\sum_i\lambda_{ij}y_i\right).
	\]
\end{proof}

%\begin{corollary}
%	Todo elemento no nulo de $U\otimes V$ puede escribirse como una suma finita
%	$\sum_{i=1}^N u_i\otimes v_i$ para un conjuntos $\{u_i:1\leq i\leq
%	N\}\subseteq U$ y $\{v_i:1\leq i\leq N\}\subseteq V$ linealmente
%	independientes.
%\end{corollary}
%
%\begin{proof}
%	tomar $N$ minimal	
%\end{proof}

\index{Producto tensorial!de álgebras}
El siguiente lema nos permite definir el \textbf{producto tensorial de
álgebras}.

\begin{lemma}
	Si $A$ y $B$ son álgebras, entonces $A\otimes B$ es un álgebra con el
	producto
	\[
		(a\otimes b)(x\otimes y)=ax\otimes by.
	\]
\end{lemma}

\begin{proof}
	Para $x\in A$, $y\in B$ consideramos $R_x\otimes R_y\in\End_K(A\otimes B)$.
	Como la función $A\times B\to\End_K(A\otimes B)$, $(x,y)\mapsto R_x\otimes
	R_y$, es bilineal, existe una función lineal $\varphi\colon A\otimes
	B\to\End_K(A\otimes B)$, $\varphi(x\otimes y)=R_x\otimes R_y$. Para $u,v\in A\otimes B$ definimos
	\[
		uv=\varphi(v)(u).
	\]
	Esta operación es bilineal pues por ejemplo
	\[
		u(v+w)=\varphi(v+w)(u)=(\varphi(v)+\varphi(w))(u)=\varphi(v)(u)+\varphi(w)(u)=uv+uw.
	\]
	Además
	$(a\otimes b)(x\otimes y)=\varphi(x\otimes y)(a\otimes b)=(R_x\otimes R_y)(a\otimes b)=ax\otimes by$.
	Un cálculo sencillo muestra que este producto es asociativo.
\end{proof}

\begin{exercise}
	Demuestre que para álgebras valen las siguientes afirmaciones:
	\begin{enumerate}
		\item $A\otimes B\simeq B\otimes A$.
		\item $(A\otimes B)\otimes C\simeq A\otimes(B\otimes C)$.
		\item $A\otimes K\simeq A\simeq K\otimes A$.
		\item Si $A\otimes A'$ y $B\otimes B'$ entonces $A\otimes B\simeq A'\otimes B'$.
	\end{enumerate}
\end{exercise}

Veamos algunos ejemplos:

\begin{proposition}
	Si $G$ y $H$ son grupos, entonces $K[G]\otimes K[H]\simeq K[G\times H]$.
\end{proposition}

\begin{proof}
	Sabemos que $\{g\otimes h:g\in G,h\in H\}$ es una base de $K[G]\otimes K[H]$ y que
	$G\times H$ es una base de $K[G\times H]$. Tenemos entonces un isomorfismo lineal 
	\[
	K[G]\otimes K[H]\to K[G\times H], 
	\quad 
	g\otimes h\mapsto (g,h),
	\]
	que además es multiplicativo. Luego $K[G]\otimes K[H]\simeq K[G\times H]$
	como álgebras.
\end{proof}

\begin{proposition}
	Si $A$ es un álgebra, entonces $A\otimes K[X]\simeq A[X]$.	
\end{proposition}

\begin{proof}
	Todo elemento de $A\otimes K[X]$ se escribe unívocamente como una suma
	finita de la forma $\sum a_i\otimes X^i$. Un cálculo sencillo muestra que
	$A\otimes K[X]\mapsto A[X]$, $\sum a_i\otimes X^i\mapsto \sum a_iX^i$, es
	un isomorfismo de álgebras.
\end{proof}

\begin{exercise}
	Demuestre que si $A$ es un álgebra, $A\otimes M_n(K)\simeq M_n(A)$. En
	particular, $M_n(K)\otimes M_m(K)\simeq M_{nm}(K)$.
\end{exercise}

\index{Extensión de escalares}
Estos últimos dos ejemplos son casos particulares de una construcción
importante que involucra productos tensoriales y se conoce como
\textbf{extensión de escalares}.

\begin{theorem}
	Sea $A$ un álgebra sobre $K$ y sea $E$ una extensión de $K$. Entonces
	$A^E=E\otimes_KA$ es un álgebra sobre $E$ con respecto a la multiplicación
	por escalares dada por
	\[
		\lambda(\mu\otimes a)=(\lambda\mu)\otimes a,
	\]
	para $\lambda,\mu\in E$ y $a\in A$.
\end{theorem}

\begin{proof}
	Sea $\lambda\in E$. Como la función $E\times A\to E\otimes_KA$,
	$(\mu,a)\mapsto (\lambda\mu)\otimes a$, es $K$-bilineal, existe una
	transformación lineal $E\otimes_KA\to E\otimes_KA$, $\mu\otimes a\mapsto
	(\lambda\mu)\otimes a$. Queda bien definida entonces la multiplicación por
	escalares y además 
	\[
	\lambda(u+v)=\lambda u+\lambda v
	\]
	para $\lambda\in E$ y $u,v\in E\otimes_KA$. Un cálculo directo muestra que además 
	\[
	(\lambda+\mu)u=\lambda u+\mu u,
	\quad
	(\lambda\mu)u=\lambda(\mu u),
	\quad
	\lambda(uv)=(\lambda u)v=u(\lambda v)
	\]
	valen para todo $u,v\in E\otimes_KA$ y $\lambda,\mu\in E$.
\end{proof}

\begin{exercise}
	Demuestre que valen las siguientes afirmaciones:
	\begin{enumerate}
		\item $1\otimes A$ es una subálgebra de $A^E$ isomorfa a $A$.
		\item Si $\{a_i:i\in I\}$ es base de $A$, entonces $\{1\otimes a_i:i\in
			I\}$ es base de $A^E$.
	\end{enumerate}
\end{exercise}

\begin{exercise}
	Demuestre que si $G$ es un grupo y $K$ es un subcuerpo de $E$, entonces
	$E\otimes_K K[G]\simeq E[G]$.
\end{exercise}

Estamos en condiciones de demostrar el teorema de Formanek:

\begin{theorem}[Formanek]
	\index{Teorema!de Formanek}
	Sea $K$ un cuerpo de característica cero y sea $G$ un grupo. Si todo
	elemento de $K[G]$ es inversible o un divisor de cero, entonces $G$ es
	localmente finito.
\end{theorem}

\begin{proof}
	Como $K$ es de característica cero, $\Q\subseteq K$ y $K[G]\simeq
	K\otimes_{\Q}\Q[G]$. Todo $\beta\in K\otimes_{\Q}\Q[Q]$ se escribe
	unívocamente como 
	\[
		\beta=1\otimes\beta_0+\sum k_i\otimes\beta_i,
	\]
	donde $\{1,k_1,k_2,\dots,\}$ es una base de $K$ como $\Q$-espacio
	vectorial. Sea $\alpha\in\Q[G]$ y sea $\beta\in K[G]$ tal que $\alpha\beta=1$. Como entonces 
	\[
	1\otimes 1=(1\otimes\alpha)\beta=1\otimes \alpha\beta_0+\sum k_i\otimes \alpha\beta_i,
	\]
	la unicidad de la escritura nos dice que $\alpha\beta_0=1$. De la misma
	forma, si $\alpha\beta=0$, entonces $\alpha\beta_j=0$ para todo $j$. Luego,
	como todo $\alpha\in\Q[G]$ es inversible o un divisor de cero, el resultado
	se obtiene al usar el teorema~\ref{thm:FormanekQ} de Formanek para $\Q$.
\end{proof}

\section*{Rickart's theorem}

En esta sección vamos a demostrar que para cualquier grupo $G$ el radical de
Jacobson de $\C[G]$ es cero. Demostraremos también que el radical de Jacobson
de $\R[G]$ es cero.

\begin{definition}
	\index{Anillo!con involución}
	\index{Involución!de un anillo}
	Sea $R$ un anillo. Una \textbf{involución} del anillo $R$ es un morfismo
	aditivo $R\to R$, $x\mapsto x^*$, tal que $x^{**}=x$ y $(xy)^*=y^*x^*$ para
	todo $x,y\in R$.
\end{definition}

De la definición se deduce inmediatamente que si $R$ es unitario, entonces
$1^*=1$.

\begin{example}
	La conjugación $z\mapsto\overline{z}$ es una involución de $\C$.
\end{example}

\begin{example}
	La trasposición $X\mapsto X^T$ es una involución del
	anillo $M_n(K)$.
\end{example}

\begin{example}
	Sea $G$ un grupo. Entonces
	$\left(\sum_{g\in G}\alpha_gg\right)^*=\sum_{g\in G}\overline{\alpha_g}g^{-1}$ 
	es una involución de $\C[G]$.
\end{example}

Dado un grupo $G$, se define la \textbf{traza} de un elemento
$\alpha=\sum_{g\in G}\alpha_gg\in K[G]$ como $\trace(\alpha)=\alpha_1$. Es
fácil ver que $\trace\colon K[G]\to K$, $\alpha\mapsto\trace(\alpha)$ es una
función $K$-lineal tal que $\trace(\alpha\beta)=\trace(\beta\alpha)$.

\begin{exercise}
	Sea $G$ un grupo finito y $K$ un cuerpo tal que su característica no divide al orden de $G$.
	Demuestre las siguientes afirmaciones:
	\begin{enumerate}
		\item Si $\alpha\in K[G]$ es nilpotente, entonces $\trace(\alpha)=0$.
		\item Si $\alpha\in K[G]$ es idempotente, entonces $\trace(\alpha)=\dim
			K[G]\alpha/|G|$.
	\end{enumerate}
\end{exercise}

\begin{exercise}
	Demuestre que 
	$\langle\alpha,\beta\rangle=\trace(\alpha\beta^*)$, $\alpha,\beta\in\C[G]$, 
	define un producto interno en $\C[G]$.
\end{exercise}

\begin{lemma}
	\label{lem:algebraico}
	Sea $G$ un grupo. Si $J(\C[G])\ne 0$, entonces existe $\alpha\in J(\C[G])$ tal que 
	$\trace(\alpha^{2^m})\in\R_{\geq1}$ 
	para todo $m\geq1$.
\end{lemma}

\begin{proof}
	Sea $\alpha=\sum_{g\in G}\alpha_gg\in\C[G]$. Entonces	
	\[
		\trace(\alpha^*\alpha)
		=\sum_{g\in G}\overline{\alpha_g}\alpha_g
		=\sum_{g\in G}|\alpha_g|^2\geq|\alpha_1|^2
		=|\trace(\alpha)|^2.
	\]
	Al usar esta fórmula para algún $\alpha$ tal que $\alpha^*=\alpha$ y usar
	inducción se obtiene que $\trace(\alpha^{2^m})\geq|\trace(\alpha)|^{2^m}$
	para todo $m\geq1$. 

	Sea $\beta=\sum_{g\in G}\beta_gg\in J(\C[G])$ tal que $\beta\ne0$. Como
	$\trace(\beta^*\beta)=\sum_{g\in G}|\beta_g|^2\ne0$ y $J(\C[G])$ es un ideal, 
	\[
		\alpha=\frac{\beta^*\beta}{\trace(\beta^*\beta)}\in J(\C[G]).
	\]
	Este elemento $\alpha$ cumple que $\alpha^*=\alpha$ y $\trace(\alpha)=1$.
	Luego $\trace(\alpha^{2^m})\geq 1$ para todo $m\geq1$.
\end{proof}

El ejercicio~\ref{exa:norma} implica que $\C[G]$ con
$\dist(\alpha,\beta)=|\alpha-\beta|$ es un espacio métrico. En este espacio
métrico, la función $\C[G]\to\C$, $\alpha\mapsto \trace(\alpha)$, es una
función continua.

\begin{lemma}
	\label{lem:phi_diferenciable}
	Sea $\alpha\in J(\C[G])$. La función
	\[
		\varphi\colon\C\to\C[G],\quad
		\varphi(z)=(1-z\alpha)^{-1},
	\]
	es continua, diferenciable y $\varphi(z)=\sum_{n\geq0}\alpha^nz^n\in\C[G]$ si $|z|$
	es suficientemente pequeño.
\end{lemma}

\begin{proof}	
	Sean $y,z\in\C$. Como $\varphi(y)$ y $\varphi(z)$ conmutan, 
	\begin{equation}
		\label{eq:Rickart}
		\begin{aligned}
			\varphi(y)-\varphi(z)&=\left( (1-z\alpha)-(1-y\alpha)\right)(1-y\alpha)^{-1}(1-z\alpha)^{-1}\\
			&=(y-z)\alpha\varphi(y)\varphi(z).
		\end{aligned}
	\end{equation}
	Entonces $|\varphi(y)|\leq|\varphi(z)|+|y-z||\alpha\varphi(y)||\varphi(z)|$ y luego
	\[
		|\varphi(y)|\left( 1-|y-z||\alpha\varphi(z)|\right)\leq|\varphi(z)|.
	\]
	Fijado $z$ podemos elegir $y$ suficientemente cerca de $z$ de forma tal que
	se cumpla que  $1-|y-z||\alpha\varphi(z)|\geq1/2$. Luego
	$|\varphi(y)|\leq2|\varphi(z)|$. De la igualdad~\eqref{eq:Rickart} se
	obtiene entonces $|\varphi(y)-\varphi(z)|\leq2|y-z||\alpha||\varphi(z)|^2$
	y luego $\varphi$ es una función continua. Por la
	expresión~\eqref{eq:Rickart}, 
	\[
	\varphi'(z)
	=\lim_{y\to z}\frac{\varphi(y)-\varphi(z)}{y-z}
	=\lim_{y\to z}\alpha\varphi(y)\varphi(z)
	=\alpha\varphi(z)^2
	\]
	para todo $z\in\C$.

	Si $z$ es tal que $|z||\alpha|=|z\alpha|<1$, entonces 
	\[
		\varphi(z)-\sum_{n=0}^Nz^n\alpha^n
		=\varphi(z)\left(1-(1-z\alpha)\sum_{n=0}^Nz^n\alpha^n\right)
		=\varphi(z)(z\alpha)^{N+1}
	\]
	y luego
	\[
		\left|\varphi(z)-\sum_{n=0}^Nz^n\alpha^n\right|\leq|\varphi(z)||z\alpha|^{N+1}.
	\]
	Como $\varphi(z)$ está acotada cerca de $z=0$, se concluye que
	$\left|\varphi(z)-\sum_{n=0}^Nz^n\alpha^n\right|\to0$ si $N\to\infty$.
\end{proof}

Estamos en condiciones de demostrar el teorema de Rickart:

\begin{theorem}[Rickart]
	\index{Teorema!de Rickart}
	Si $G$ es un grupo, entonces $J(\C[G])=0$.
\end{theorem}

\begin{proof}
	Sea $\alpha\in J(\C[G])$ y sea $\varphi(z)=(1-\alpha z)^{-1}$. Sea 
	$f\colon\C\to \C$ dada por
	$f(z)=\trace\varphi(z)=\trace\left((1-z\alpha)^{-1}\right)$. Por el lema~\ref{lem:phi_diferenciable},
	$f(z)$ es una función entera tal que $f'(z)=\trace(\alpha\varphi(z)^2)$ y
	\begin{equation}
		\label{eq:Taylor}
		f(z)=\sum_{n=0}^\infty z^n\trace(\alpha^n)
	\end{equation}
	si $|z|$ es suficientemente pequeño. En particular, la
	igualdad~\eqref{eq:Taylor} es la expansión en serie de Taylor para $f(z)$
	en el origen. Esto implica que esta serie tiene radio de convergencia
	infinito y converge a $f(z)$ para todo $z\in\C$. En particular,
	\begin{equation}
		\label{eq:limite}
		\lim_{n\to\infty}\trace(\alpha^n)=0.
	\end{equation}
	Por otro lado, si $\alpha\ne0$ el lema~\ref{lem:algebraico} implica que
	$\trace(\alpha^{2^m})\geq1$ para todo $m\geq0$, lo que contradice el límite
	calculado en~\eqref{eq:limite}. Luego $\alpha=0$.
\end{proof}

Para demostrar un corolario necesitamos dos lemas:

\begin{lemma}[Nakayama]
	\label{lem:Nakayama}
	\index{Lema!de Nakayama}
	Sea $R$ un anillo unitario y sea $M$ un $R$-módulo finitamente generado. Si
	$J(R)M=M$, entonces $M=0$.
\end{lemma}

\begin{proof}
	Supongamos que $M$ está generado por los elementos $x_1,\dots,x_n$. Como $x_n\in M=J(R)M$, 
	existen $r_1,\dots,r_n\in J(R)$ tales que $x_n=r_1x_1+\cdots+r_nx_n$, es decir
	$(1-r_n)x_n=\sum_{j=1}^{n-1}r_jx_j$. 
	Como $1-r_n$ es inversible, existe $s\in R$ tal que $s(1-r_n)=1$. Luego
	$x_n=\sum_{j=1}^{n-1}sr_jx_j$ 
	y entonces $M$ está generado por $x_1,\dots,x_{n-1}$. Al repetir este
	procedimiento una cierta cantidad finita de veces, se obtiene que $M=0$.
\end{proof}

\begin{lemma}
	\label{lem:Rickart}
	Sea $\iota\colon R\to S$ un morfismo de anillos unitarios. Si 
	\[
	S=\iota(R)x_1+\cdots+\iota(R)x_n,
	\]
	donde cada $x_j$ cumple que $x_jy=yx_j$ para todo $y\in\iota(R)$, entonces
	$\iota(J(R))\subseteq J(S)$.
\end{lemma}

\begin{proof}
	Veamos que $J=\iota(J(R))$ actúa trivialmente en cada $S$-módulo simple $M$.
	Si $M$ es un $S$-módulo simple, escribimos $M=Sm$ para algún $m\ne0$. Es
	claro que $M$ es un $R$-módulo con $r\cdot m=\iota(r)m$. Como
	\[
		M=Sm=(\iota(R)x_1+\cdots+\iota(R)x_n)m=\iota(R)(x_1m)+\cdots+\iota(R)(x_nm),
	\]
	$M$ es finitamente generado como $\iota(R)$-módulo. Además $J(R)\cdot
	M=JM=\iota(J)M$ es un $S$-submódulo de $M$ pues
	\[
		x_j(JM)=(x_jJ)M=(Jx_j)M=J(x_jM)\subseteq JM.
	\]
	Como $M\ne0$, el lema de Nakayama implica que $J(R)\cdot M\subsetneq M$. Luego,
	como $M$ es un $S$-módulo simple, se concluye que $J(R)M=0$.
\end{proof}

\begin{corollary}
	Si $G$ es un grupo, entonces $J(\R[G])=0$. 
\end{corollary}

\begin{proof}
	Sea $\iota\colon \R[G]\to\C[G]$ la inclusión canónica. Como 
	\[
	\C[G]=\R[G]+i\R[G],
	\]
	el lema~\ref{lem:Rickart} y el teorema de Rickart implican que
	$\iota(J(\R[G]))\subseteq J(\C[G])=0$. Luego $J(\R[G])=0$ pues $\iota$ es
	inyectiva. 
\end{proof}
